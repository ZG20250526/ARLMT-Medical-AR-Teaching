(llava) guoyunfei@tme-GS4840:~/projects/LLaVA-main/llava$ TRANSFORMERS_OFFLINE=1 HF_HUB_OFFLINE=1 . finetune.sh 
[2024-07-29 11:33:07,782] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:09,309] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-29 11:33:09,309] [INFO] [runner.py:571:main] cmd = /home/guoyunfei/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py --deepspeed /home/guoyunfei/projects/LLaVA-main/scripts/zero2.json --model_name_or_path /data0/GYF/offline_model/liuhaotian/llava-v1.6-mistral-7b --version plain --data_path /data0/GYF/data/llava-med/data/select1millionpictures2024072801.json --image_folder /data0/GYF/data/llava-med/data/images --vision_tower /data0/GYF/offline_model/openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.6-7b-pretrain/mm_projector.bin --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/llava-v1.6-7b_plain-finetune --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-07-29 11:33:12,021] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:14,010] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-29 11:33:14,010] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-29 11:33:14,010] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-29 11:33:14,010] [INFO] [launch.py:163:main] dist_world_size=8
[2024-07-29 11:33:14,010] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-29 11:33:17,848] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:18,327] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:18,361] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:18,458] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:18,481] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:18,488] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:18,495] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:18,507] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 11:33:19,232] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 11:33:19,889] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 11:33:19,897] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 11:33:19,955] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 11:33:19,990] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 11:33:20,023] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 11:33:20,086] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 11:33:20,086] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-29 11:33:20,130] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:20<00:00,  5.02s/it]
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:20<00:00,  5.18s/it]
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:20<00:00,  5.25s/it]
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:21<00:00,  5.37s/it]
Loading checkpoint shards:  75%|████████████████████████████████████████████████████████████                    | 3/4 [00:21<00:08,  8.05s/it]openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:21<00:00,  5.47s/it]
Formatting inputs...Skip in lazy mode
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:22<00:00,  5.50s/it]
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:21<00:00,  5.44s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:21<00:00,  5.39s/it]
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
wandb: Currently logged in as: james20201124 (james20201124-temple-university). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/guoyunfei/projects/LLaVA-main/llava/wandb/run-20240729_113459-nsger0cj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-puddle-7
wandb: ⭐️ View project at https://wandb.ai/james20201124-temple-university/huggingface
wandb: 🚀 View run at https://wandb.ai/james20201124-temple-university/huggingface/runs/nsger0cj
  0%|                                                                                                                  | 0/60 [00:00<?, ?it/s]/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
{'loss': 1.7304, 'learning_rate': 1e-05, 'epoch': 0.02}                                                                                       
{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 0.03}                                                                                          
{'loss': 0.0, 'learning_rate': 1.998533413851124e-05, 'epoch': 0.05}                                                                          
{'loss': 0.0, 'learning_rate': 1.9941379571543597e-05, 'epoch': 0.07}                                                                         
{'loss': 0.0, 'learning_rate': 1.9868265225415263e-05, 'epoch': 0.08}                                                                         
{'loss': 0.0, 'learning_rate': 1.976620555710087e-05, 'epoch': 0.1}                                                                           
{'loss': 0.0, 'learning_rate': 1.963549992519223e-05, 'epoch': 0.12}                                                                          
{'loss': 0.0, 'learning_rate': 1.9476531711828027e-05, 'epoch': 0.13}                                                                         
{'loss': 0.0, 'learning_rate': 1.9289767198167918e-05, 'epoch': 0.15}                                                                         
{'loss': 0.0, 'learning_rate': 1.9075754196709574e-05, 'epoch': 0.17}                                                                         
{'loss': 0.0, 'learning_rate': 1.883512044446023e-05, 'epoch': 0.18}                                                                          
{'loss': 0.0, 'learning_rate': 1.8568571761675893e-05, 'epoch': 0.2}                                                                          
{'loss': 0.0, 'learning_rate': 1.827688998156891e-05, 'epoch': 0.22}                                                                          
{'loss': 0.0, 'learning_rate': 1.796093065705644e-05, 'epoch': 0.23}                                                                          
{'loss': 0.0, 'learning_rate': 1.7621620551276366e-05, 'epoch': 0.25}                                                                         
{'loss': 0.0, 'learning_rate': 1.725995491923131e-05, 'epoch': 0.27}                                                                          
{'loss': 0.0, 'learning_rate': 1.6876994588534234e-05, 'epoch': 0.28}                                                                         
{'loss': 0.0, 'learning_rate': 1.647386284781828e-05, 'epoch': 0.3}                                                                           
{'loss': 0.0, 'learning_rate': 1.6051742151937655e-05, 'epoch': 0.32}                                                                         
{'loss': 0.0, 'learning_rate': 1.5611870653623826e-05, 'epoch': 0.33}                                                                         
{'loss': 0.0, 'learning_rate': 1.515553857177022e-05, 'epoch': 0.35}                                                                          
{'loss': 0.0, 'learning_rate': 1.4684084406997903e-05, 'epoch': 0.37}                                                                         
{'loss': 0.0, 'learning_rate': 1.4198891015602648e-05, 'epoch': 0.38}                                                                         
{'loss': 0.0, 'learning_rate': 1.3701381553399147e-05, 'epoch': 0.4}                                                                          
{'loss': 0.0, 'learning_rate': 1.31930153013598e-05, 'epoch': 0.42}                                                                           
{'loss': 0.0, 'learning_rate': 1.2675283385292212e-05, 'epoch': 0.43}                                                                         
{'loss': 0.0, 'learning_rate': 1.2149704402110243e-05, 'epoch': 0.45}                                                                         
{'loss': 0.0, 'learning_rate': 1.161781996552765e-05, 'epoch': 0.47}                                                                          
{'loss': 0.0, 'learning_rate': 1.1081190184239418e-05, 'epoch': 0.48}                                                                         
{'loss': 0.0, 'learning_rate': 1.0541389085854177e-05, 'epoch': 0.5}                                                                          
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 0.52}                                                                                          
{'loss': 0.0, 'learning_rate': 9.458610914145826e-06, 'epoch': 0.53}                                                                          
{'loss': 0.0, 'learning_rate': 8.918809815760585e-06, 'epoch': 0.55}                                                                          
{'loss': 0.0, 'learning_rate': 8.382180034472353e-06, 'epoch': 0.57}                                                                          
{'loss': 0.0, 'learning_rate': 7.85029559788976e-06, 'epoch': 0.58}                                                                           
{'loss': 0.0, 'learning_rate': 7.324716614707794e-06, 'epoch': 0.6}                                                                           
{'loss': 0.0, 'learning_rate': 6.806984698640202e-06, 'epoch': 0.62}                                                                          
{'loss': 0.0, 'learning_rate': 6.298618446600856e-06, 'epoch': 0.63}                                                                          
{'loss': 0.0, 'learning_rate': 5.801108984397355e-06, 'epoch': 0.65}                                                                          
{'loss': 0.0, 'learning_rate': 5.3159155930021e-06, 'epoch': 0.67}                                                                            
{'loss': 0.0, 'learning_rate': 4.844461428229782e-06, 'epoch': 0.68}                                                                          
{'loss': 0.0, 'learning_rate': 4.388129346376177e-06, 'epoch': 0.7}                                                                           
{'loss': 0.0, 'learning_rate': 3.948257848062351e-06, 'epoch': 0.72}                                                                          
{'loss': 0.0, 'learning_rate': 3.5261371521817247e-06, 'epoch': 0.73}                                                                         
{'loss': 0.0, 'learning_rate': 3.123005411465766e-06, 'epoch': 0.75}                                                                          
{'loss': 0.0, 'learning_rate': 2.740045080768694e-06, 'epoch': 0.77}                                                                          
{'loss': 0.0, 'learning_rate': 2.3783794487236367e-06, 'epoch': 0.78}                                                                         
{'loss': 0.0, 'learning_rate': 2.0390693429435626e-06, 'epoch': 0.8}                                                                          
{'loss': 0.0, 'learning_rate': 1.7231100184310955e-06, 'epoch': 0.82}                                                                         
{'loss': 0.0, 'learning_rate': 1.4314282383241097e-06, 'epoch': 0.83}                                                                         
{'loss': 0.0, 'learning_rate': 1.1648795555397719e-06, 'epoch': 0.85}                                                                         
{'loss': 0.0, 'learning_rate': 9.242458032904311e-07, 'epoch': 0.87}                                                                          
{'loss': 0.0, 'learning_rate': 7.102328018320859e-07, 'epoch': 0.88}                                                                          
{'loss': 0.0, 'learning_rate': 5.234682881719766e-07, 'epoch': 0.9}                                                                           
{'loss': 0.0, 'learning_rate': 3.645000748077709e-07, 'epoch': 0.92}                                                                          
{'loss': 0.0, 'learning_rate': 2.3379444289913344e-07, 'epoch': 0.93}                                                                         
{'loss': 0.0, 'learning_rate': 1.317347745847386e-07, 'epoch': 0.95}                                                                          
{'loss': 0.0, 'learning_rate': 5.862042845640403e-08, 'epoch': 0.97}                                                                          
{'loss': 0.0, 'learning_rate': 1.4665861488761813e-08, 'epoch': 0.98}                                                                         
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 1.0}                                                                                             
{'train_runtime': 495.3702, 'train_samples_per_second': 15.368, 'train_steps_per_second': 0.121, 'train_loss': 0.02884006897608439, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [08:05<00:00,  8.10s/it]
[2024-07-29 11:43:24,688] [INFO] [launch.py:347:main] Process 1380621 exits successfully.
[2024-07-29 11:43:25,690] [INFO] [launch.py:347:main] Process 1380618 exits successfully.
[2024-07-29 11:43:25,690] [INFO] [launch.py:347:main] Process 1380615 exits successfully.
[2024-07-29 11:43:25,691] [INFO] [launch.py:347:main] Process 1380620 exits successfully.
[2024-07-29 11:43:25,691] [INFO] [launch.py:347:main] Process 1380616 exits successfully.
[2024-07-29 11:43:26,692] [INFO] [launch.py:347:main] Process 1380619 exits successfully.
[2024-07-29 11:43:26,693] [INFO] [launch.py:347:main] Process 1380617 exits successfully.
wandb: \ 0.046 MB of 0.046 MB uploaded
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:              train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:            train/learning_rate ▅███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁
wandb:                     train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 60
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.0
wandb:               train/total_flos 1.7371820077141197e+17
wandb:               train/train_loss 0.02884
wandb:            train/train_runtime 495.3702
wandb: train/train_samples_per_second 15.368
wandb:   train/train_steps_per_second 0.121
wandb: 
wandb: 🚀 View run cool-puddle-7 at: https://wandb.ai/james20201124-temple-university/huggingface/runs/nsger0cj
wandb: ⭐️ View project at: https://wandb.ai/james20201124-temple-university/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240729_113459-nsger0cj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[2024-07-29 11:43:58,725] [INFO] [launch.py:347:main] Process 1380614 exits successfully.

