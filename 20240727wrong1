(llava) guoyunfei@tme-GS4840:~/projects/LLaVA-main/llava$ . pretrain.sh 
[2024-07-27 16:41:22,145] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:24,128] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-27 16:41:24,128] [INFO] [runner.py:571:main] cmd = /home/guoyunfei/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py --deepspeed /home/guoyunfei/projects/LLaVA-main/scripts/zero2.json --model_name_or_path /data0/GYF/offline_model/liuhaotian/llava-v1.6-vicuna-7b --version plain --data_path /data0/GYF/data/llava/LLaVA-CC3M-Pretrain-595K/blip_laion_cc_sbu_558k.json --image_folder /data0/GYF/data/llava/LLaVA-CC3M-Pretrain-595K/images/images --vision_tower /data0/GYF/offline_model/openai/clip-vit-large-patch14-336/preprocessor_config.json --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /home/guoyunfei/projects/LLaVA-main/llava/checkpoints/llava-v1.6-7b-pretrain --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-07-27 16:41:26,833] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:28,960] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-27 16:41:28,960] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-27 16:41:28,960] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-27 16:41:28,960] [INFO] [launch.py:163:main] dist_world_size=8
[2024-07-27 16:41:28,960] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-27 16:41:32,538] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:32,587] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:32,695] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:32,903] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:33,187] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:33,240] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:33,604] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:33,752] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 16:41:33,861] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 16:41:34,048] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 16:41:34,287] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 16:41:34,307] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 16:41:34,777] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 16:41:34,871] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 16:41:34,996] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 16:41:35,292] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 16:41:35,293] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py", line 385, in cached_file
    resolved_file = hf_hub_download(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1848, in _raise_on_head_call_error
    raise LocalEntryNotFoundError(
huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3594, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 46, in __init__
    self.model = LlavaLlamaModel(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 38, in __init__
    super(LlavaLlamaModel, self).__init__(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/llava_arch.py", line 35, in __init__
    self.vision_tower = build_vision_tower(config, delay_load=True)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/builder.py", line 13, in build_vision_tower
    return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/clip_encoder.py", line 20, in __init__
    self.load_model()
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/clip_encoder.py", line 29, in load_model
    self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name, local_files_only=True)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 206, in from_pretrained
    image_processor_dict, kwargs = cls.get_image_processor_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 335, in get_image_processor_dict
    resolved_image_processor_file = cached_file(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py", line 425, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like openai/clip-vit-large-patch14-336 is not the path to a directory containing a file named preprocessor_config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-07-27 16:41:38,982] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066864
[2024-07-27 16:41:39,184] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066865
[2024-07-27 16:41:39,626] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066866
[2024-07-27 16:41:39,879] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066867
[2024-07-27 16:41:39,879] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066868
[2024-07-27 16:41:40,116] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066869
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py", line 385, in cached_file
    resolved_file = hf_hub_download(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1848, in _raise_on_head_call_error
    raise LocalEntryNotFoundError(
huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3594, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 46, in __init__
    self.model = LlavaLlamaModel(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 38, in __init__
    super(LlavaLlamaModel, self).__init__(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/llava_arch.py", line 35, in __init__
    self.vision_tower = build_vision_tower(config, delay_load=True)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/builder.py", line 13, in build_vision_tower
    return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/clip_encoder.py", line 20, in __init__
    self.load_model()
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/clip_encoder.py", line 29, in load_model
    self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name, local_files_only=True)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 206, in from_pretrained
    image_processor_dict, kwargs = cls.get_image_processor_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 335, in get_image_processor_dict
    resolved_image_processor_file = cached_file(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py", line 425, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like openai/clip-vit-large-patch14-336 is not the path to a directory containing a file named preprocessor_config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py", line 385, in cached_file
    resolved_file = hf_hub_download(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1848, in _raise_on_head_call_error
    raise LocalEntryNotFoundError(
huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3594, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 46, in __init__
    self.model = LlavaLlamaModel(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 38, in __init__
    super(LlavaLlamaModel, self).__init__(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/llava_arch.py", line 35, in __init__
    self.vision_tower = build_vision_tower(config, delay_load=True)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/builder.py", line 13, in build_vision_tower
    return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/clip_encoder.py", line 20, in __init__
    self.load_model()
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/multimodal_encoder/clip_encoder.py", line 29, in load_model
    self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name, local_files_only=True)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 206, in from_pretrained
    image_processor_dict, kwargs = cls.get_image_processor_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 335, in get_image_processor_dict
    resolved_image_processor_file = cached_file(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py", line 425, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like openai/clip-vit-large-patch14-336 is not the path to a directory containing a file named preprocessor_config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[2024-07-27 16:41:40,405] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066870
[2024-07-27 16:41:40,609] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066871
[2024-07-27 16:41:40,774] [ERROR] [launch.py:321:sigkill_handler] ['/home/guoyunfei/.conda/envs/llava/bin/python', '-u', '/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py', '--local_rank=7', '--deepspeed', '/home/guoyunfei/projects/LLaVA-main/scripts/zero2.json', '--model_name_or_path', '/data0/GYF/offline_model/liuhaotian/llava-v1.6-vicuna-7b', '--version', 'plain', '--data_path', '/data0/GYF/data/llava/LLaVA-CC3M-Pretrain-595K/blip_laion_cc_sbu_558k.json', '--image_folder', '/data0/GYF/data/llava/LLaVA-CC3M-Pretrain-595K/images/images', '--vision_tower', '/data0/GYF/offline_model/openai/clip-vit-large-patch14-336/preprocessor_config.json', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', '/home/guoyunfei/projects/LLaVA-main/llava/checkpoints/llava-v1.6-7b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1

