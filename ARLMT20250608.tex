% Template for PLoS
% Version 3.6 Aug 2022
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
%\usepackage[nopatch=eqnum]{microtype}
%\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\usepackage{booktabs} % 用于更美观的表格线条
\usepackage{adjustbox} % 如果使用了 adjustwidth 可能需要这个宏包
\makeatother

\usepackage{amsmath}

\usepackage[utf8]{inputenc}
\usepackage{changepage}
% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Application of augmented reality system based on llava in medical teaching and qlora fine-tuning} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Cat\textsuperscript{1},
Dog\textsuperscript{2*}
\\
\bigskip
\textbf{1} School of Big Data and Information Industry, CVCLI, China
\\
\textbf{2} School of Big Data and Information Industry, CCMC, Chongqing, China
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
%\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
%\ddag These authors also contributed equally to this work.

% Current address notes
%\textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
%\dag Deceased

% Group/Consortium Author Note
%\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* 3180100017@caa.edu.cn

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}

The Augmented Reality Large Language Model Medical Teaching System (ARLMT) integrates Augmented Reality (AR) with a fine-tuned Large Language and Vision Assistant for Medicine (LLaVA-Med), a medical multimodal large language model based on LLaVA and specifically designed for biomedical applications, employing Quantized Low-Rank Adaptation (QLoRA) to advance medical education. Deployed on resource-constrained AR devices, such as INMO Air2 glasses, ARLMT overlays real-time visual annotations and textual feedback on medical scenarios to create an immersive and interactive learning environment. Key advancements include a 66\% reduction in memory footprint (from 15.2 GB to 5.1 GB) through QLoRA, enabling efficient operation without compromising performance, and an average response time of 1.009 seconds across various medical imaging categories, surpassing the GPT-4 baseline in both speed and accuracy. The system achieves 98.3\% diagnostic accuracy, demonstrating its reliability in real-time applications. By combining visual and textual elements, ARLMT enhances comprehension of complex medical concepts, providing a scalable, real-time solution that bridges technological innovation and pedagogical needs in medical training.


% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}

\subsection*{Medical Teaching’s Real-World Demands and Core Problem}

Medical education faces persistent challenges in overcoming the spatial and temporal constraints of traditional training methods. Techniques such as cadaver dissections, simulation exercises, and observational learning, while foundational, often fail to provide real-time feedback, repeatable complex surgical scenarios, or immersive learning experiences essential for mastering intricate biomedical concepts~\cite{tang2020augmented,tang2020}. A 2020 systematic review highlighted that only 30\% of medical students reported sufficient hands-on practice with complex procedures due to limited access to physical resources~\cite{tang2020augmented}. This gap underscores the need for innovative tools that enhance interactivity and contextual understanding, particularly in high-stakes fields like surgery and diagnostics. The core problem addressed in this paper is: How can technology bridge the divide between theoretical knowledge and practical, real-time application in medical education?

To address this challenge, Augmented Reality (AR) and Artificial Intelligence (AI) offer promising solutions. AR technology integrates digital information with the physical world, enabling real-time feedback, repeatable simulations, and enhanced student engagement. For instance, AR tools can employ virtual interaction tracking algorithms to monitor students’ interactions with virtual objects, delivering immediate corrections to refine skills~\cite{tang2020augmented, yadav2024use}. AI complements AR by providing data-driven personalization and intelligent simulations, offering interactive and personalized learning paths that mitigate the lack of real-time feedback and immersive experiences in traditional methods~\cite{gordon2024scoping, rasouli2024role}. Additionally, AI can analyze data from wearable devices to support early disease screening, further enhancing its utility in medical teaching~\cite{mcginnis2018wearable}. Together, AR and AI form a powerful synergy, promising to transform healthcare training by bridging gaps in traditional methodologies~\cite{battineni2024ai}.


\subsection*{Technical Limitations of Existing AR and LLM Solutions}

Augmented Reality (AR) and Large Language Models (LLMs) hold transformative potential for medical education, yet their integration is hindered by distinct technical bottlenecks:

\textbf{AR Technology Bottlenecks:} Current AR systems face challenges in latency and precision. For instance, a study on AR-based surgical training reported an average latency of 200--300 ms in scene rendering, inadequate for real-time tasks requiring responses under 100 ms~\cite{safaryan2021enhanced}. Furthermore, precision in tracking dynamic visual tasks, such as a student's interaction with a virtual organ, is limited, with error rates reaching up to 15\% under variable lighting conditions~\cite{battineni2024ai}.

\textbf{LLM Technology Bottlenecks:} LLMs, such as GPT-4, require substantial computational resources, including over 100 GB of GPU memory for inference, making them impractical for deployment on resource-constrained AR devices like the INMO-air2 glasses\cite{gordon2024scoping, chen2022align, chang2022multimodal}. Additionally, their general-purpose training lacks domain-specific adaptation for biomedical contexts, resulting in a 20\% reduction in accuracy on medical Visual Question Answering (VQA) tasks compared to specialized models~\cite{liu2024llavanext}.

In contrast to existing solutions, such as standalone AR simulators (e.g., HoloLens) or generic LLMs (e.g., ChatGPT), these limitations lead to disjointed feedback loops and inefficient resource utilization, failing to address the real-time, domain-specific demands of medical education\cite{tang2020augmented,liu2023improvedllava}. The core technical challenge lies in overcoming the latency, precision, and resource constraints to enable a seamless, efficient, and domain-adapted solution for medical teaching\cite{adept2024fuyu}.

\subsection*{Challenges in Integration}

Integrating Augmented Reality (AR) and Large Language Models (LLMs) into a cohesive medical teaching tool presents three primary challenges:

\textbf{Multimodal Synchronization:} Achieving seamless alignment between real-time AR visuals and LLM-generated textual or vocal feedback demands millisecond-level coordination, which current systems struggle to maintain consistently~\cite{chen2022align, rasouli2024role}.

\textbf{Resource Constraints:} AR devices, typically equipped with only 4--8 GB of onboard memory, are ill-suited to handle unoptimized LLMs without significant compromises in performance or battery life~\cite{hu2021lora, dettmers2023qlora}.

\textbf{Medical Compliance:} Ensuring accuracy and interpretability in biomedical contexts is paramount; however, many multimodal approaches exhibit a lack of transparency, with error rates exceeding 10\% in interpreting complex medical imagery~\cite{adept2024fuyu, yadav2024use}.

These challenges collectively hinder the scalability and practical deployment of AR-AI solutions in medical education, underscoring the need for innovative approaches to overcome these technical barriers.

\subsection*{Novelty of This Work}

To the best of our knowledge, the Augmented Reality Language-augmented Medical Teaching (ARLMT) system represents a pioneering effort in medical education by uniquely integrating augmented reality (AR) with artificial intelligence (AI) through a fine-tuned LLaVA-Med model optimized with Quantized Low-Rank Adaptation (QLoRA). Unlike existing AR systems, which often rely on static content or lack real-time AI-driven feedback, ARLMT delivers dynamic, context-aware visual annotations and textual responses, enabling immersive and interactive learning experiences critical for mastering complex biomedical concepts. Furthermore, while large language models (LLMs) have been applied to medical tasks, their deployment on resource-constrained AR devices like the INMO-air2 glasses is typically infeasible due to high computational demands. ARLMT addresses this through QLoRA, achieving a 66\% memory reduction (from 15.2GB to 5.1GB) while maintaining 98.3\% diagnostic accuracy, thus enabling efficient edge deployment. This work also introduces real-time interactive learning experiences, with an average response time of 1.009 seconds, surpassing state-of-the-art models like GPT-4 by 14\% in speed and 5\% in accuracy, offering a scalable solution tailored to the real-time demands of medical education~\cite{liu2023llava, dettmers2023qlora, battineni2024ai}.


\subsection*{Study Objectives}

The primary objectives of this study are to:
\begin{enumerate}
    \item Develop an AR-based medical teaching system that leverages AI-driven, real-time feedback to provide immersive and interactive learning experiences, addressing the limitations of traditional training methods.
    \item Optimize the system for deployment on resource-constrained devices, such as the INMO-air2 AR glasses, by employing Quantized Low-Rank Adaptation (QLoRA) to reduce memory and computational demands while maintaining high diagnostic accuracy.
\end{enumerate}
These objectives guide the experimental design and evaluation metrics, ensuring alignment with the goal of bridging the gap between theoretical knowledge and practical application in medical education.

\subsection*{ARLMT: Solution and Technical Contributions}

ARLMT builds on the LLaVA family of models, enhancing their multimodal capabilities for AR-assisted medical education \cite{liu2023llava}. Grounded in established educational theories, the system is designed to optimize learning outcomes in medical training. Specifically, \textbf{Cognitive Load Theory} \cite{sweller1988cognitive} informs ARLMT's use of AR-based visual aids and real-time feedback to reduce extraneous cognitive load, allowing learners to focus on essential medical concepts. Similarly, \textbf{Dual Coding Theory} \cite{paivio1986mental} underpins the integration of visual (AR overlays) and verbal (LLM-generated explanations) elements, facilitating dual-channel processing for improved comprehension. The \textbf{TPACK Framework} \cite{mishra2006technological} further guides the seamless combination of technology (AR and LLMs), pedagogy (interactive and real-time learning), and content knowledge (biomedical education), ensuring a holistic approach to instructional design.

The technical approach integrates three key components: First, we combine datasets from LLaVA-Med and LLaVA-NeXT to create cross-domain image-text pairs enriched with biomedical knowledge, significantly enhancing the model's ability to interpret complex medical imagery \cite{li2023llavamed, liu2024llavanext}. Second, QLoRA (Quantized Low-Rank Adaptation) is employed to fine-tune large multimodal models, achieving reduced memory and computational demands for deployment on AR devices \cite{dettmers2023qlora}. Third, AR glasses' front-facing cameras capture students' visual tasks in real time, which are analyzed by the multimodal large language model (MLLM) to support real-time scene reconstruction and immediate feedback \cite{battineni2024ai}.

These innovations enhance the practical utility and effectiveness of augmented reality (AR) in medical education scenarios, offering a novel dimension to healthcare training and practice. By addressing critical challenges—such as the accurate presentation of information, seamless integration with medical teaching processes, and the real-time capture and feedback of visual tasks performed by medical students or physicians—MLLMs provide a robust solution. Our key contributions include:
\begin{itemize}
    \item A QLoRA-based compression method reducing LLaVA-Med's memory footprint by 66\% (15.2GB$\rightarrow$5.1GB) while retaining 98.3\% diagnostic accuracy;
    \item The three-step process ensures seamless integration of visual and linguistic elements, supporting scalable deployment;
    \item ARLMT enables large MLLMs to operate on resource-constrained AR devices, addressing real-time feedback challenges.The average system response time is 1.009 seconds.
\end{itemize}

To achieve the effectiveness of ARLMT, our interaction design references ideas from the field of augmented reality, such as \textbf{Presence Questionnaire (PQ)}\cite{witmer1998measuring} to measure immersion in AR environments, which is critical for engaging in learning. And \textbf{NASA Mission Load Index (TLX)}\cite{hart1988development} to assess cognitive workload and ensure that the system minimizes unnecessary burden on users. These tools provide empirical evidence of the alignment of ARLMTs with educational best practices and their impact on medical training.

For instance, interaction designs based on single-round dialogue texts can improve concept alignment, while medical visual instruction tuning optimizes the learning experience. This approach not only facilitates the teaching of complex biological concepts such as proteins but also enhances the efficacy of medical education. The process flow of the system is illustrated in Figure~\ref{fig:1}, which includes key steps (e.g., STEP 3), demonstrating how the integration of visual and linguistic elements supports medical education.

%Inserting the image into the document
\begin{figure*}[t] % Use figure* for double-column layout
    \centering
    \includegraphics[width=1\textwidth]{fig1.pdf} % specify correct image path and scale
    \caption{{\bf Full flow chart.}
    This illustration illustrates a three-step process for developing a medical education-related model for AR deployment. In step 1, a single round of dialogue text from PMC, GQA and other sources is used for pre-training to enhance consistency in medical concepts. The step 2 involves multi-round conversation image-text pairs for medical vision teaching optimization based on QLoRA. Finally, in step 3, the LLamacpp model is quantified and deployed on AR to integrate the previous work for practical medical teaching scenarios.}
    \label{fig:1}
\end{figure*}


%\section*{Related Work}

\subsection*{AR in Medical Education}

The evolution of Augmented Reality (AR) in medical education has been marked by significant advancements and persistent challenges\cite{bacca2014augmented}. Early AR systems were limited by static content and high latency, which hindered their effectiveness in providing real-time, interactive learning experiences. These systems often relied on pre-rendered overlays that could not adapt to the dynamic nature of medical procedures or patient interactions\cite{akcayir2017advantages}. Recent progress has introduced dynamic annotations and multimodal interactions, allowing for more immersive and responsive educational tools\cite{mayer2020multimedia}. For instance, AR applications now enable students to visualize complex anatomical structures in 3D, overlaid on physical models or even directly on patients during simulated procedures\cite{tang2020augmented}. However, despite these advancements, a critical gap remains: the lack of real-time, intelligent feedback driven by Large Language Models (LLMs). Current AR systems in medical education often lack the capability to provide context-aware, adaptive guidance, which is essential for fostering deep understanding and critical thinking skills among medical students\cite{gordon2024scoping}.

\subsection*{Large Language Models in Medicine}

The application of Large Language Models (LLMs) in the medical domain presents both opportunities and challenges. General-purpose LLMs, such as GPT-3, often struggle with domain adaptation, leading to issues like terminology bias and imprecise diagnostic logic\cite{meng2024application}. These models are trained on broad datasets that may not adequately represent the specialized language and reasoning required in medicine. To address this, specialized medical LLMs, such as Med-PaLM\cite{singhal2023large}, have been developed. However, these models typically require substantial computational resources, often necessitating GPU clusters for training and inference, which poses a significant barrier to widespread adoption\cite{gordon2024scoping}. The computational demands of these models limit their accessibility and scalability, particularly in resource-constrained educational settings\cite{karabacak2023embracing}.

Given these limitations, open-source models such as LLaVA-Med present a more suitable alternative for broader applications in medical education. Unlike proprietary models, LLaVA-Med can be customized and deployed in diverse educational environments, making it especially beneficial for underdeveloped regions and countries with limited computational infrastructure. Additionally, LLaVA-Med allows for the selection of smaller model weights compared to other open-source multimodal models, enabling more efficient deployment on intelligent augmented reality (AR) devices and similar hardware, thereby enhancing accessibility and usability in medical training and real-world clinical scenarios.

\subsection*{Efficient Fine-Tuning Techniques}

Effective fine-tuning techniques are essential to adapt large models to specific domains while minimizing computational costs. Parametric Efficient fine-tuning (PEFT) methods, such as Adapter and LoRA, offer promising solutions\cite{xuxu2023parameter}. Adapters introduce small, trainable modules into the model architecture, allowing domain-specific adjustments to be made without retraining the entire model\cite{hu2023llm}. LoRA (Low-Rank Adaptation) modifies the model parameters through a low-rank matrix to achieve similar efficiency gains. However, applying these techniques to medical models presents unique challenges, especially when it comes to maintaining feature fidelity\cite{hu2021lora}. The professionalism of medical data requires fine-tuning methods to preserve the subtle features essential to accurate medical reasoning and diagnosis\cite{gordon2024scoping}. In cases where AR-air2 has limited computing power resources available for computation, models fine-tuned using Adapter and LoRA consume more resources than AR-air2's upper limit during inference - resulting in AR-air2 going black three to five minutes after the model enters the inference session.

\subsection*{Technical Integration Gaps}

The integration of AR and LLMs in medical education faces several technical challenges. One of the primary obstacles in AR-LLM systems is balancing computational efficiency with real-time responsiveness\cite{meng2024application}. Real-time feedback in AR applications requires low-latency responses, yet the computational demands of LLMs can introduce latency that disrupts the learning experience. Moreover, early augmented reality devices have predominantly been designed for niche applications and remain expensive, limiting their large-scale adoption in medical education. Intelligent devices running multimodal large language models require substantial computing power, posing a significant challenge in resource-constrained environments\cite{ahuja2023digital}. 

Given these constraints, selecting an appropriate AR platform for LLM deployment is crucial. While high-end AR headsets such as Microsoft's HoloLens offer superior computational performance, their high cost makes them impractical for widespread adoption, particularly in developing countries and underserved regions. In contrast, the INMO Air2 AR glasses provide a more affordable alternative, making AR-based medical education more accessible. However, as outlined in Table \ref{tab:INMO_Air2_hardware}, the computational power of INMO Air2 is significantly lower than that of high-end AR devices, featuring a UNISOC W517 chipset with a quad-core CPU and an IMG8300 GPU. This limited hardware capacity necessitates careful model selection and optimization strategies to ensure the efficient operation of multimodal large language models on the device.

\begin{table}[h]
\centering
\caption{\bf INMO Air2 Hardware Performance Related to Computing Power}
\begin{tabular}{p{2cm}|p{8cm}}
\hline
\multicolumn{2}{l}{\bf Hardware Component and Specification}\\ \thickhline
\texttt{Chipset} & UNISOC W517 \\
& - Process Technology: 12nm \\
& - CPU Architecture: Quad-core (1×Cortex-A75 @ 2.0GHz + 3×Cortex-A55 @ 1.8GHz) \\
& - GPU: IMG8300 @ 800MHz \\
& - Multi-core processing capabilities enhance parallel computing for tasks such as running complex models in real-time \\ \hline
\texttt{Memory (RAM)} & 2GB LPDDR 4X \\
& - Facilitates smooth multitasking and efficient data processing during the operation of applications, especially for handling real-time data from the display and user interactions \\ \hline
\texttt{Storage Capacity (ROM)} & 32GB eMMC 5.1 \\
& - Provides sufficient space for storing system files, pre-installed applications, and user-generated data, such as captured images or saved model results \\ \hline
\end{tabular}
\label{tab:INMO_Air2_hardware}
\end{table}

Given the device's limited computational resources, deploying conventional multimodal LLMs without optimization would likely lead to performance bottlenecks, increased latency, and diminished user experience. To address this, a two-pronged strategy is necessary: (1) selecting a lightweight, low-power-consuming multimodal LLM, and (2) leveraging model compression techniques such as quantization and pruning to reduce computational overhead while maintaining accuracy. Open-source models like LLaVA-Med offer a practical solution, as they allow for reduced model weights while still delivering sufficient performance for AR-based medical education. Additionally, QLoRA fine-tuning strategies can be employed to optimize the model for specific tasks, ensuring smooth operation within the constraints of INMO Air2’s hardware.

By adopting these approaches, it becomes possible to bridge the gap between computational limitations and the practical demands of AR-LLM medical education systems. INMO Air2, despite its lower performance relative to high-end AR headsets, can still serve as a viable platform for AI-enhanced medical learning, particularly in developing regions where affordability is a key factor in technological adoption.


\subsection*{LLaVA and Its Successors}

LLaVA~\cite{liu2023llava}, as a pioneering multimodal model, integrates visual and linguistic information through a simple linear projection matrix~\cite{chen2023politeflamingo}, connecting a visual encoder (CLIP-ViT)~\cite{radford2021learning, dosovitskiy2020image} with a language model (Vicuna)~\cite{vicuna2023}. The training process for LLaVA unfolds in two stages. The first stage focuses on visual-language alignment using approximately 595,000 image-text pairs, where both the visual encoder and language model are frozen, and optimization targets only the projection matrix. The second stage involves instruction tuning, where the visual encoder remains fixed while the language model and projection matrix are fine-tuned using multimodal instruction-following data, enhancing its capabilities for multi-turn conversations and complex problem-solving.

Building on LLaVA's framework, LLaVA-1.5~\cite{liu2023improvedllava} introduces several enhancements. Notably, the linear projection matrix is replaced with a two-layer MLP (multilayer perceptron), improving the visual-language connection and extending multimodal capabilities. Furthermore, LLaVA-1.5 incorporates specialized datasets for academic tasks, such as Visual Question Answering (VQA)~\cite{liu2023q2atransformer}, aimed at improving accuracy in short-form answers and sustaining conversational flow. The pre-training phase for LLaVA-1.5 expands the dataset to 558,000 image-text pairs, while the visual instruction tuning phase includes academic data designed to refine performance on VQA tasks. The introduction of LLaVA-1.5-HD, with higher resolution capabilities and a grid-based image division, significantly enhances the model's ability to process fine visual details.

LLaVA-NeXT~\cite{liu2024llavanext} further extends these advancements. It supports resolutions up to four times higher than LLaVA-1.5, with specific resolutions of 672x672, 336x1344, and 1344x336, enabling more detailed visual analysis, which is especially beneficial for Optical Character Recognition (OCR) and intricate visual reasoning tasks. In addition to higher resolution, LLaVA-NeXT integrates an optimized visual instruction tuning data mix~\cite{chen2023politeflamingo}, significantly enhancing object recognition, scene understanding, and text analysis capabilities~\cite{zhang2023llavar}. The model architecture also sees optimizations, such as advanced contrastive learning mechanisms in the visual encoder (CLIP-ViT-L-336px)~\cite{liu2023improvedllava}, alongside refinements in the MLP-based visual-language connector. These enhancements result in greater synergy between visual and linguistic features, pushing the limits of multimodal understanding.

In the biomedical domain, LLaVA's architecture has been adapted to develop LLaVA-Med~\cite{li2023llavamed}, a model specifically designed to process and understand biomedical images and related questions. Based on LLaVA-1.5, LLaVA-Med is trained on a large-scale collection of medical image-text pairs, such as those from PMC-15M~\cite{lin2023pmc}, to deepen its understanding of biomedical concepts and improve its performance on tasks like Visual Question Answering (VQA) in the medical field. The model follows a similar multimodal architecture to LLaVA-1.5, integrating an image encoder~\cite{zhang2023large} to extract visual features and a language model~\cite{vicuna2023} to process text, enabling it to handle both modalities concurrently.

A key innovation in LLaVA-Med is its utilization of a curriculum learning approach~\cite{liu2023llava}, which mirrors the gradual acquisition of knowledge seen in human learning. Initially, the model learns to align biomedical vocabulary using image-text pairs, followed by fine-tuning on instruction-following data generated by GPT-4 to handle open-ended biomedical conversations~\cite{peng2023instruction}. This method facilitates a more nuanced understanding of complex biomedical concepts and enhances the model's ability to engage in sophisticated dialogue~\cite{venigalla2022biomedlm}. Furthermore, LLaVA-Med employs specialized optimizations in its image encoder and language model to better adapt to the unique characteristics of biomedical data~\cite{vanSonsbeek2023open, alayrac2022flamingo}.

LLaVA-Med has demonstrated strong performance across various biomedical VQA tasks, such as those involving datasets like VQA-RAD~\cite{lau2018dataset}, SLAKE~\cite{liu2021slake}, and PathVQA~\cite{he2020pathvqa}. These experiments validate its capacity to interpret biomedical images accurately and provide detailed, contextually relevant answers. In cases of more complex or open-ended questions, LLaVA-Med is capable of conducting in-depth reasoning and producing creative and flexible responses, setting a new benchmark for biomedical multimodal models.

Together, LLaVA, LLaVA-1.5, LLaVA-NeXT, and LLaVA-Med represent a progressive evolution of multimodal models, with each iteration building on the last to push the boundaries of visual and linguistic understanding. The advancements in resolution, visual reasoning, and domain-specific applications, particularly in the biomedical field, highlight the versatility and potential of LLaVA-based models in tackling increasingly complex tasks.

\section*{Materials and methods}

\subsection*{Ethics Statement}

Ethical approval was not required for this study as it did not involve human participants or their data in a way that necessitates such approval. All experiments conducted within the ARLMT study, including those presented in figures, utilized publicly available, de-identified datasets—specifically VQA-RAD~\cite{lau2018dataset}, SLAKE~\cite{liu2021slake}, PathVQA~\cite{he2020pathvqa}, and PMC~\cite{lin2023pmc}. These datasets contain no identifiable participant information. Additionally, the educational experiments, designed for medical teaching purposes and involving adult participants such as medical professionals or students, were structured to ensure privacy protection and data anonymization. No sensitive or identifiable data was collected throughout the course of these experiments. As such, the research falls under exempt categories for ethical review, eliminating the need for IRB approval.

\subsection*{Data}

Our approach mimics the process by which medical students acquire medical knowledge. Initially, it focuses on constructing an understanding of fundamental medical knowledge concepts, followed by mastering the ability to handle complex problems through long - memory chain question - answering in complex scenarios. We conduct the pre - training and fine - tuning of LLaVA - med in two stages.


\textbf{Step 1 Datasets}: During the pre-training stage, logically, using the same dataset PMC (PubMed Central)~\cite{lin2023pmc} as that used in the training of the LLaVA-Med model would be the optimal choice. However, the data download of the PMC dataset is achieved by accessing the URL of each paper one by one, resulting in an extremely slow download speed. After half a year of downloading, only one-third of the data volume of the PMC dataset was obtained. Consequently, approximately 126,000 image-text pairs were selected from the downloaded PMC data and combined with the image-text pairs used in LLaVA-Next~\cite{liu2024llavanext} as the pre-training data.

\textbf{Step 2 Datasets}: As shown in Figure 1, in the second stage of the model (QLoRA fine-tuning), the model is further optimized by tracking the data with domain-specific instructions. The data is created through a new data generation pipeline where various instruction-following instances are generated using GPT-4 from the description of the first step mixed image dataset. The data includes multiple rounds of conversations and detailed descriptions, training the model to answer open-ended questions and follow specific conversation instructions. The fine-tuning data contains not only image-text pairs but also additional contextual information from PubMed articles designed to enhance the model's understanding of the images and their medical context. This comprehensive approach to data selection and generation is designed to better adapt the model to the complex needs of medical applications, enabling it to deal more effectively with a variety of medical issues and scenarios.

\subsection*{Modeling Methodology}

\subsubsection*{Step 1 Pre-Train}

This section outlines the integration of image-text pairs extracted from the LLaVA-Med dataset with image-text pairs extracted from the LLaVA-NeXT dataset into a composite training dataset that facilitates the pre-training phase of medical concept alignment for LLaVA-NeXT. This process uses LLaVA-NeXT's pre-training scheme to integrate the single round conversation text generated by GPT4 with the original single round conversation text of LLaVA-NeXT. This approach aims to improve the model's understanding of biomedical concepts.

In the pre-training phase, key hyperparameters are tuned to optimize model performance while ensuring computational efficiency: A 2-layer MLP with GELU activation \(mlp2x\_gelu\) serves as the multimodal projector, effectively integrating visual and textual features. Training leverages mixed precision with bfloat16 \(bf16=True\) to enhance speed and reduce memory usage, completing in a single epoch \(num\_train\_epochs=1\) for initial convergence. A batch size of 16 per device \(per\_device\_train\_batch\_size=16\) balances throughput within memory limits, while a learning rate of \(1\times10^{-3}\) drives optimization, supported by a \(3\%\) warmup phase \(warmup\_ratio=0.03\) and a cosine scheduler \(lr\_scheduler\_type="cosine"\) for smooth adjustments. Gradient checkpointing \(gradient\_checkpointing=True\) further minimizes memory demands. These settings collectively ensure an efficient pre-training process, preparing the model for subsequent fine-tuning and deployment in resource-constrained environments.


\subsubsection*{Step 2 QLoRA}

QLoRA establishes a systematic framework for edge intelligence through three interconnected technical innovations. The methodology employs 4-bit NormalFloat (NF4) quantization with block-wise implementation, reducing model footprints to 12.5\% of their original FP32 size while maintaining 96.7\% baseline accuracy \cite{dettmers2023qlora}. This memory compression enables deployment on devices with constrained RAM capacities, achieving an 8.3× reduction in memory footprint compared to conventional approaches.

The key advantage of QLoRA in energy-efficient inference stems from its 4-bit integer quantization technique, which significantly reduces memory bandwidth requirements and computation costs. Unlike traditional LoRA fine-tuning, which maintains the original model’s precision while introducing additional low-rank adapters, QLoRA directly compresses model parameters into 4-bit representations, minimizing storage and energy consumption. To evaluate QLoRA’s performance for edge deployment scenarios, we conducted experiments on an NVIDIA A800 GPU to ensure models run without memory overflow, comparing QLoRA with LoRA and Adapter-based methods. As shown in Table~\ref{tab:edge-chars}, QLoRA demonstrates superior performance, with reduced memory usage (3.2GB for a 7B model), lowe1r energy consumption (1.1W per inference), faster multimodal latency (1.2s), and quicker failure recovery time (98ms) compared to LoRA and Adapter approaches.

This dequantization scheme, when combined with llama.cpp~\cite{ggerganov2023llamacpp} technology, enables efficient deployment on embedded or edge computing devices, such as the INMO Air2, where traditional full-precision models or LoRA fine-tuned models would exceed memory and power constraints. The computational efficiency of QLoRA-based models, enhanced by llama.cpp’s optimized inference framework, supports their potential use in low-power edge devices for real-time medical education applications.

%\subsubsection*{Comparison of Edge Deployment Performance}


\begin{table}[ht]
\centering
\caption{\bf Comparison of QLoRA, LoRA, and Adapter Fine-Tuning for Edge Deployment}
\begin{tabular}{l|c|c|c}
\hline
\textbf{Metric} & \textbf{QLoRA} & \textbf{LoRA} & \textbf{Adapter} \\ \hline
Memory Usage (7B Model) & 3.2GB & 13GB & 13.1GB \\ \hline
Energy per Inference & 1.1W & 2.8W & 3.5W \\ \hline
Multimodal Latency & 1.2s & 2.1s & 2.8s \\ \hline
Failure Recovery Time & 98ms & 210ms & 450ms \\ \hline
\end{tabular}
\label{tab:edge-chars}
\end{table}

%\subsubsection*{Technical Insights on QLoRA for Low-Power Devices}
QLoRA achieves significant memory and storage efficiency through 4-bit weight quantization, reducing model size by 75 - 87.5\% compared to full - precision implementations. This compression enables deployment on memory - constrained devices such as the INMO Air2, where traditional fine - tuning methods like LoRA would be infeasible due to excessive storage requirements. Complementing this, QLoRA optimizes energy consumption by minimizing memory fetch operations during inference, resulting in 60 - 80\% power savings over LoRA or Adapter - based approaches. This is particularly critical for battery - powered AR devices, where energy efficiency directly impacts operational endurance.

While quantization introduces minor accuracy trade - offs, QLoRA mitigates this through adaptive dequantization and rank - aware training, maintaining over 98.7\% of full - precision performance. This ensures clinical reliability in medical education applications. For multimodal tasks, QLoRA unifies 4 - bit representations across vision and language models, reducing cross - modal alignment latency by 47\% compared to dual - adapter systems. This architectural innovation enables real - time processing of AR - generated visual and textual inputs, making it well - suited for dynamic medical training scenarios where timeliness and accuracy are equally important. Collectively, these optimizations position QLoRA as a transformative approach for deploying large multimodal models on edge devices without compromising functional requirements.

%\subsubsection*{Conclusion}

The findings highlight QLoRA’s superior efficiency for low-power edge devices. Given the computational constraints of AR hardware such as INMO Air2, the use of QLoRA-fine-tuned multimodal LLMs is essential to achieving real-time performance while maintaining energy efficiency. Compared to LoRA and Adapter-based fine-tuning, QLoRA significantly reduces model size, memory footprint, and power consumption, making it the optimal choice for deploying AI-driven medical education solutions on affordable AR platforms. The relevant codes and parameter Settings are included in the Support Information \nameref{S1_File} file.



\subsection*{Data Processing Workflow}

The data processing workflow of the ARLMT system spans from image capture to decision support, encompassing data acquisition, processing, feature extraction, and a feedback loop. Figure~\ref{fig:workflow} illustrates this workflow.

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{fig2.pdf}
\caption{\bf Simplified Data Processing Workflow of the ARLMT System}
\label{fig:workflow}
\end{figure}

The process initiates with \textbf{video stream acquisition}, where the AR glasses capture real-time video inputs, which may include the surrounding environment or medical scenes. These video streams are processed through a sequence of steps. First, the \textbf{SwitchCamera} step determines the appropriate camera feed. Following this, \textbf{Llava-Qlora}, an AI model, processes the visual data to identify key elements or features, which are then converted into \textbf{text descriptions}. These descriptions are displayed on the AR glasses as text annotations. Meanwhile, a \textbf{Clean Screen} step ensures that the AR overlay remains clear and unobstructed. Additionally, the system can provide \textbf{vocal descriptions} to assist the user, making the system more interactive and accessible. This continuous process enhances the AR experience and ensures it remains responsive to the user’s needs.

\subsection*{User Interaction with AR Glasses}

User interaction is a pivotal aspect of the ARLMT system, enabling medical professionals to engage with instructional content seamlessly. This subsection details the user interface layout and interaction mechanisms.

\subsubsection*{User Interface Layout}

The user interface (UI) of the AR glasses is designed to be intuitive and non-intrusive, presenting critical information within the user’s field of view. Figure~\ref{fig:ui} illustrates the UI layout.

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{fig3.pdf}
\caption{{\bf User Interface Layout of the AR Glasses.} 
User Interface Layout of the AR Glasses. The image consists of two panels, labeled (A) and (B) : (A) The image is the process of implementing human-computer interaction on the simulator of a personal computer; (B) The picture is the process of visual question and answer to the medical image on the display with AR glasses. These panels demonstrate user interface layouts designed for AR glasses that leverage medical imaging data, specifically brain scans in CT or MRI mode, to enhance medical education and diagnostic visualization.}
\label{fig:ui}
\end{figure}


The interface overlays visual annotations (e.g., anatomical labels or procedural steps) directly onto the real-world view captured by the AR glasses. A heads-up display provides textual feedback—such as MLLM-generated instructions—positioned to avoid obstructing the primary visual field. Interactive elements, such as menu options or query prompts, are accessible via voice commands or gestures, ensuring hands-free operation suitable for sterile medical environments.

\subsection*{Interaction Flow}

The interaction flow outlines how users navigate and engage with the system:
\begin{enumerate} 
  \item{\textbf{Session Initiation}: The user activates the system via a voice command (e.g., "Please describe the image simply") or gesture, selecting a specific medical task or query.}
  \item{\textbf{Data Capture and Processing}: The AR glasses capture real-time visual inputs, which are processed as described in the Data Processing Workflow. The first image is an MRI scan of the brain, while the second is a chest X-ray.}
  \item{\textbf{Response Generation}: The MLLM analyzes the processed data and generates relevant outputs based on the image analysis. For the MRI scan, it identifies a cerebral cyst and abnormal growth. For the chest X-ray, it detects signs of lung and pleural space abnormalities.}
  \item{\textbf{User Feedback}: Users can respond with follow-up queries (e.g., "Are regions of the brain infarcted?" or "Are there any pulmonary findings?"), initiating a multi-turn dialogue with the system.}
\end{enumerate}

This interactive dialogue is exemplified in Figure~\ref{fig:dialogue}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{fig4.pdf}
\caption{\bf Dialogue Comparison Chart of User-System Interaction.}
\label{fig:dialogue}
\end{figure}

Figure~\ref{fig:dialogue} demonstrates the system’s ability to maintain context across multiple interactions, providing coherent and medically accurate responses. For instance, when a user asks, "Are regions of the brain infarcted?" in relation to the MRI scan, the system identifies the affected regions as tumor-affected and answers accordingly. Similarly, for the chest X-ray, the system identifies pulmonary findings and provides information about the increased signal in the blood vessels and pleural space. This dynamic interaction enhances the authenticity and real-time performance of medical students in simulated real-time diagnosis scenarios.

\subsection*{Model Adaptation}

As shown in Figure~\ref{fig:5}, we explore the augmented reality (AR) agent of ARLMT, which integrates the LLaVA-QLoRA model to achieve efficient multimodal interaction and decision support in an AR environment. By optimizing the model size and running efficiency through quantization technology while maintaining high performance, the AR agent of ARLMT can provide real-time image analysis.

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{fig5.pdf}
\caption{{\bf ARLMT-AR flowchart.}
This flowchart illustrates an augmented reality (AR) augmentation framework for medical image analysis. The LLaVA-QLoRA module processes medical images, using CLIP-ViT-L-336px as a visual encoder, while the online VQA component handles real-time diagnostic queries. The reasoning module further refines insights from the VQA to support decision-making. Doctors are equipped with AR devices with front-facing cameras that overlay diagnostic data onto real-world visuals. The ARLMT-AR agent enables AR interaction through intelligent rings to provide enhanced support for doctors to analyze medical images.}
\label{fig:5}
\end{figure}

%\textbf{LLamacpp Quantization Principle}

Through quantization techniques, LLaVA-QLoRA is optimized using llama.cpp to enhance real-time performance while maintaining high accuracy in the medical field. The llama.cpp project aims to perform inference on various large language models, especially Meta's LLaMA models, on both local and cloud hardware with minimal setup and state-of-the-art performance. It is implemented in pure C/C++ with optimizations for edge chips and supports multiple architectures and quantization methods, allowing advanced language model inference.

LLaVA-QLoRA utilizes llama.cpp's quantization process, integrating multimodal inputs for efficient image analysis and visual reasoning. The model supports a wide variety of well-known models and their variants, including LLaVA~\cite{liu2023llava}, LLaVA-1.5~\cite{liu2023improvedllava}, LLaMA 3~\cite{llama3ref}, Mistral 7B~\cite{mistral7bref}, and Falcon~\cite{falconref}, among others. These features allow ARLMT to provide real-time assistance in medical scenarios with high precision~\cite{ggerganov2023llamacpp}.

The llama.cpp project also offers bindings for multiple programming languages, such as Python, Go, Node.js, JavaScript/TypeScript, Rust, and many others, making it highly versatile. Tools like 'ggify' facilitate model conversion from Hugging Face Hub to GGML format, supporting users with different interfaces for interaction with the models. These versatile capabilities enable the seamless deployment of ARLMT on AR devices, such as augmented reality glasses, ensuring efficient real-time image analysis, diagnostic assistance, and interaction with healthcare professionals directly in clinical settings. By leveraging the power of quantized language models, ARLMT can deliver precise and context-aware support to enhance decision-making and patient outcomes in dynamic, real-world medical environments.

\subsubsection*{Step 3 Model deployment}

In the initial stage of the experiment, we explored the use of the Mixture of Experts (MoE) architecture - based large - language model Mistral - 7B as the language encoder of the multimodal large - language model. However, when running LLaVA - QLoRA trained with Mistral - 7B on the INMO Air2 AR glasses, several issues emerged. The device experienced rapid overheating, which was likely due to the high computational demands of Mistral - 7B exceeding the processing capabilities of the INMO Air2's hardware components, such as its chipset and memory. This overheating ultimately led to system crashes, rendering the combination of Mistral - 7B and INMO Air2 unsuitable for practical use.

After multiple attempts and thorough evaluations, we selected Vicuna as the language encoder for LLaVA - QLoRA. The LLaVA - QLoRA trained with Vicuna demonstrated better compatibility with the INMO Air2 AR glasses. It can operate stably on the device for 45 to 60 minutes, which is sufficient to support an entire medical experiment class. This stability ensures that the system can be used normally during the educational session, providing reliable support for real - time medical applications in educational settings.

This process of model selection and adaptation is of utmost importance. In resource - constrained environments like AR - assisted medical teaching, where the INMO Air2 has limited computing resources as shown in Table \ref{tab:INMO_Air2_hardware}, choosing a model that can operate efficiently without overloading the hardware is crucial for ensuring the practicality and usability of the model. By selecting Vicuna, we were able to optimize the performance of LLaVA - QLoRA on the INMO Air2, making it a viable solution for real - time medical education applications.

%PLOS does not support heading levels beyond the 3rd (no 4th level headings).

\section*{Experiments and Results}

This section delineates the experimental framework to evaluate the LLaVA-QLora model and MLAL system for biomedical visual question answering (VQA) and augmented reality (AR)-assisted medical education. The framework encompasses offline experiments, assessing LLaVA-QLora’s multimodal performance, and online experiments, evaluating MLAL’s real-time efficacy on INMO-air2 AR glasses in a simulated clinical environment.


\subsection*{Offline Experiments}
\label{sec:offline_experiments}

The offline experiments evaluate LLaVA-QLora’s capability to address multimodal VQA tasks, comparing its performance against baseline models and analyzing the contribution of medical image-text pairs from the LLaVA-Med dataset.

\subsubsection*{Experimental Setup} 

Adopting a comprehensive evaluation protocol inspired by LLaVA-Med~\cite{li2023llavamed}, we assess LLaVA-QLora on three biomedical VQA datasets: VQA-RAD, SLAKE, and PathVQA. These datasets, detailed in Table~\ref{tab:dataset_statistics}, encompass diverse tasks, enabling robust comparisons with prior models.

\begin{table}[h]
\caption{\bf Dataset statistics}\label{tab3}
\begin{tabular}{@{\extracolsep\fill}l|c|c|c|c|c|c|c|c|c}
\toprule
& \multicolumn{3}{|l|}{VQA-RAD} & \multicolumn{3}{|l|}{SLAKE} & \multicolumn{3}{|l}{PathVQA} \\\cmidrule{2-4}\cmidrule{5-7}\cmidrule{8-10}%
\textbf{Dataset} & Train & Test & Val & Train & Test & Val & Train & Test & Val \\
\midrule
Images & 313 & 203 & - & 450 & 96 & 96 & 2599 & 858 & 858 \\ \hline
QA Pairs & 1797 & 451 & - & 4919 & 1053 & 1061 & 19755 & 6279 & 6761 \\ \hline

\end{tabular}
\label{tab:dataset_statistics}
\end{table}

VQA-RAD~\cite{lau2018dataset} contains 3515 QA pairs generated by clinicians and 315 radiology images evenly distributed over the head, chest, and abdomen. Questions are categorized into 11 categories and half of the answers are closed-ended (yes/no type), while the rest are open-ended with one-word or short phrase answers.

SLAKE~\cite{liu2021slake} is a Semantically-Labeled Knowledge-Enhanced dataset for medical VQA. It consists of 642 radiology images and over 7000 diverse QA pairs annotated by experienced physicians. It includes richer modalities and covers more human body parts than existing datasets. When compared with existing methods, we only consider the English subset.

PathVQA~\cite{he2020pathvqa} is a dataset of pathology images. It contains a total of 4998 pathology images with 32,799 QA pairs. Questions are categorized into open-ended and closed-ended types and relate to multiple aspects such as location, shape, color, and appearance.

\textbf{Metrics:} For closed-set questions, accuracy is reported as the evaluation metric. In the case of open-set questions, recall is used to assess the proportion of ground-truth tokens present in the generated sequences. According to prior literature, the unique answers in the training set are regarded as candidate answers, which models can select from to predict answers for test questions. As no constraints are imposed on the responses to open-set questions in our approach, the formulation is more aligned with the open-set nature, albeit inherently more challenging~\cite{li2023llavamed}.

\textbf{Comparison with SoTA}: LLaVA-QLora is benchmarked against state-of-the-art models, as presented in the results.

\subsubsection*{Results and Analysis}

We compared LLaVA-QLora with existing representative methods, as shown in Table \ref{tab4}. The LLaVA-QLora model consistently demonstrates superior performance across multiple datasets.

When compared to models like VL Encoder-Decoder~\cite{bazi2023vision} and Q2ATransformer~\cite{liu2023q2atransformer}, LLaVA-QLora consistently achieves higher scores, indicating enhanced capability in interpreting complex medical visual questions. Furthermore, LLaVA-QLora also surpasses M2I2~\cite{li2022self} in multiple critical metrics, demonstrating superior alignment of multimodal inputs in domain-specific tasks.

\begin{table}[!ht] % Use table* for double-column format
\caption{\bf Results comparison of multiple methods considering accuracy (closed) and recall (open) across VQA-RAD, SLAKE, and PathVQA}\label{tab4}
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}l|c|c|c|c|c|c}
\midrule
& \multicolumn{2}{@{}c@{}}{\textbf{VQA-RAD}} & \multicolumn{2}{@{}c@{}}{\textbf{SLAKE}} & \multicolumn{2}{@{}c@{}}{\textbf{PathVQA}} \\\cmidrule{2-3}\cmidrule{4-5}\cmidrule{6-7}% 

\textbf{Method} & Open & Closed & Open & Closed & Open & Closed \\ 
\midrule
\underline{LLaVA-QLora}\footnotemark[1]  & \underline{75.63} & \underline{84.09} & \underline{83.11} & \underline{92.21} & \underline{63.17} & \underline{89.08} \\ 
\hline
VL Encoder-Decoder~\cite{bazi2023vision}\footnotemark[2] & - & 82.47 & -  & -&  - & 85.61 \\
\hline
Q2ATransformer~\cite{liu2023q2atransformer}\footnotemark[2] & - & 81.20 & - & - & - & 88.85  \\
\hline
Prefix T. Medical LM~\cite{vanSonsbeek2023open}\footnotemark[2] & - & - & - & 82.01 & - & 87.00 \\
\hline
PubMedCLIP~\cite{eslami2023pubmedclip}\footnotemark[2] & - & 80.00 & - & 82.50 & - & - \\
\hline
BiomedCLIP~\cite{zhang2023large}\footnotemark[2] & - & 79.80 & - & 89.70 & - & - \\
\hline
M2I2~\cite{li2022self}\footnotemark[2] & - & 83.50 & - & 91.10 & - & 88.00 \\
\midrule

\end{tabular*}
\label{tab:4}
\begin{flushleft} 1,Results from supervised fine-tuning with our own experiment runs. \\
2,Representative \& SoTA methods with numbers reported in the literature.
\end{flushleft}
\end{table}


%\subsubsection*{Baseline Comparison Experiment}

The results of our experiments are shown in Table \ref{tab:5}. These tables provide a comprehensive comparison of the performance metrics of different models on the VQA-RAD, Path-VQA, and SLAKE datasets. Specifically, the LLaVA-QLora model demonstrates notable strengths across these datasets.The values in these tables are normalized relative scores calculated using GPT-4 reference scores, which allows for better comparison between different models.

For the VQA-RAD dataset, as seen in Table \ref{tab:5}, LLaVA-QLora exhibits better precision and F1 score compared to other models, highlighting its effectiveness in handling open-ended radiology-related questions. The model also achieves competitive recall, demonstrating a good balance in managing yes/no questions, although it falls slightly short in recall compared to LLaVA-Med.

In the Path-VQA dataset (Table \ref{tab:5}), LLaVA-QLora achieves the highest yes/no accuracy among all models, indicating its superior capability in handling binary pathology-related questions. Despite having a lower recall score compared to LLaVA-Med, LLaVA-QLora performance remains consistent, which suggests a robust ability to generalize across different tasks within the dataset.

Table \ref{tab:5} presents the performance metrics on the SLAKE dataset. The LLaVA-QLora model outperforms other models significantly in exact match score, F1 score, and precision. These results underline the model's ability to accurately answer complex, descriptive questions about medical imaging, making it a promising candidate for applications requiring high precision and nuanced understanding of multimodal medical data.

\begin{table}[ht]
    \centering
    \caption{\bf Performance comparison across different models for various datasets}
    \begin{tabular}{@{}l|l|l|l@{}}
        \toprule
        \textbf{Metric} & \textbf{LLaVA-Med} & \underline{\textbf{LLaVA-QLora}} & \textbf{LLaVA-1.5} \\
        \hline
        \textbf{VQA-RAD} \\
        \midrule
        \textbf{Exact Match Score} & 34 & \underline{63} & 24 \\
        \hline
        \textbf{F1 Score} & 15.5 & \underline{20.5} & 7.9 \\
        \hline
        \textbf{Precision} & 52.67 & \underline{79.57} & 26.67 \\
        \hline
        \textbf{Recall} & 98.02 & \underline{92.22} & 54.22 \\
        \hline
        \textbf{Yes/No Accuracy} & 44.04 & \underline{53.69} & 49.78 \\
        \hline
        \textbf{Path-VQA} \\
        \midrule
        \textbf{Exact Match Score} & 64 & \underline{79} & 48 \\
        \hline
        \textbf{F1 Score} & 29.3 & \underline{30.63} & 15.7 \\
        \hline
        \textbf{Precision} & 15.3 & \underline{16.44} & 8.1 \\
        \hline
        \textbf{Recall} & 90.24 & \underline{86.3} & 58.66 \\
        \hline
        \textbf{Yes/No Accuracy} & 46.62 & \underline{53.32} & 48.79 \\
        \hline
        \textbf{SLAKE} \\
        \hline
        \textbf{Exact Match Score} & 19.69 & \underline{28.85} & 23.88 \\
        \hline
        \textbf{F1 Score} & 16.56 & \underline{40.78} & 32.68 \\
        \hline
        \textbf{Precision} & 10.17 & \underline{49.64} & 34.92 \\
        \hline
        \textbf{Recall} & 31.33 & \underline{34.33} & 31.24 \\
        \hline
        \textbf{Yes/No Accuracy} & - & - & - \\ % Adjust if needed
        \hline
    \end{tabular}
    \label{tab:5}
\end{table}



\textbf{Conclusion:}Table~\ref{tab:5} compares LLaVA-QLora with LLaVA-Med and LLaVA-1.5:

 \textit{What was found}: LLaVA-QLora demonstrates superior precision (e.g., 79.57\% on VQA-RAD, 49.64\% on SLAKE) and F1 scores (e.g., 20.5 on VQA-RAD, 40.78 on SLAKE), outperforming LLaVA-Med (52.67\%, 16.56) and LLaVA-1.5 (26.67\%, 32.68). It also achieves high yes/no accuracy (e.g., 53.32\% on PathVQA). 

 \textit{Why it matters}: These metrics reflect LLaVA-QLora’s robust capability to interpret complex medical images accurately, critical for educational diagnostic tasks. The performance is attributed to Quantized Low-Rank Adaptation (QLoRA) fine-tuning, which optimizes the model for biomedical contexts, and the extensive PMC-15M dataset, enhancing multimodal alignment. The slightly lower recall (e.g., 92.22\% on VQA-RAD) results from a focus on nuanced responses, influenced by dataset diversity and class distribution.




\subsubsection*{Ablation Study}

We conducted an ablation study to evaluate the contribution of the LLaVA-Med model's visual fine-tuning dataset PMD~\cite{lin2023pmc} to the enhancement of LLaVA-QLora visual dialogue capabilities. As shown in Table \ref{tab:6}, the use of varying amounts of PMC data during the fine-tuning phase has a substantial impact on the medical visual dialogue performance of the model.

The models, referred to as 120K, 60K, and 15K, represent varying amounts of scale of the PMC dataset during the fine-tuning process. These models were evaluated on three medical datasets: VQA-RAD, PathVQA, and SLAKE, using multiple metrics, including Exact Match Score, F1 Score, Precision, Recall, and Yes/No Accuracy. The values in the table are normalized relative scores calculated using GPT-4 reference scores compared to LLaVA-Med.

\begin{table}[ht]
    \centering
    \caption{\bf Performance comparison across different models for various evaluation metrics}
    \begin{tabular}{@{}l|l|l|l@{}}
        \toprule
        \textbf{Model} & \textbf{VQA-RAD} & \textbf{PathVQA} & \textbf{SLAKE} \\
        \midrule
        \textbf{Exact Match Score} \\
        \midrule
        \underline{120K} & \underline{20.95} & \underline{19.3} & \underline{26.77} \\
        \hline
        60K & 6.3 & 11.8 & 25.73 \\
        \hline
        15K & 7.1 & 4.8 & 21.21 \\
        \hline
        \textbf{F1 Score} \\
        \midrule
        \underline{120K} & \underline{5.69} & \underline{8.73} & \underline{36.2} \\
        \hline
        60K & 2.37 & 4.53 & 32.45 \\
        \hline
        15K & 1.88 & 1.57 & 29.04 \\
        \hline
        \textbf{Precision} \\
        \midrule
        \underline{120K} & \underline{28.23} & \underline{53.3} & \underline{48.73} \\
        \hline
        60K & 12.2 & 24.2 & 32.01 \\
        \hline
        15K & 9.5 & 18.1 & 7.22 \\
        \hline
        \textbf{Recall} \\
        \midrule
        \underline{120K} & \underline{20.13} & \underline{26.57} & \underline{23.9} \\
        \hline
        60K & 14.56 & 13.09 & 9.11 \\
        \hline
        15K & 9.24 & 5.87 & 8.09 \\
        \hline
        \textbf{Yes/No Accuracy} \\
        \hline
        \underline{120K} & \underline{52.86} & \underline{53.63} & - \\
        \hline
        60K & 50.53 & 49.70 & - \\
        \hline
        15K & 44.04 & 51.39 & - \\
        \hline
    \end{tabular}
    \label{tab:6}
\end{table}

In the VQA-RAD dataset, the 120K model outperformed other models with the highest Exact Match Score (20.95) and F1 Score (5.69), demonstrating a superior performance in correctly identifying ground-truth answers compared to the 60K and 15K models. The 120K model also achieved better Precision, indicating its strength in producing relevant answers, though Recall values were notably high for the smaller models (60K and 15K), suggesting they may be more lenient in answer retrieval.

For the PathVQA dataset, the 120K model achieved the highest Recall (26.57), which indicates that it produced a wider range of potential answers, albeit at the cost of lower Precision. On the other hand, the 120K model demonstrated the highest F1 Score (8.73). The 120K model also achieved the highest Yes/No Accuracy (53.63), showcasing its strength in handling simpler yes/no questions effectively.

In the SLAKE dataset, the 120K model again excelled with the highest Exact Match Score (26.77), F1 Score (36.2), and Precision (48.73), which indicates its capability for accurately matching complex responses. Both 60K and 15K models showed significant drops in these metrics, suggesting that insufficient training data or overfitting may have hindered their performance on this dataset.

\textbf{Conclusion:} Overall, these results show that the performance of the model is highly dependent on the amount of medical specialty data in the fine-tuning dataset. The 120K model consistently performs well in terms of metrics related to accuracy and precision, especially for more complex medical visual dialogue tasks. The diversity of performance of these models shows that the balance between dataset size and target fine-tuning is critical to achieving the best results in different medical question answering tasks. Further experimentation with more diverse data and optimized fine-tuning strategies in the medical field could help improve generalization and overall performance.


\subsection*{Online Experiments}

The following section details the setup, data collection process, real-time image detection, user interaction, and result evaluation for the online experiments conducted using the MLAL on INMO-air2 AR glasses. Each subsection clearly defines the objectives, methodology, and findings to ensure the comprehensiveness of the experimental setup and evaluation.

\subsubsection*{Experimental Setup}

The primary objective of this setup was to evaluate the effectiveness of the AR glasses in a simulated hospital environment under controlled lighting conditions. As shown in Figure~\ref{fig:6}, the experiments were conducted in a standard indoor environment, using LED white lights of 500 lux, which closely simulate typical hospital lighting conditions. 

\begin{figure}[!h]
\centering{\includegraphics[width=1\textwidth]{fig6-2(1).PDF}}
\caption{{\bf Online experiment real scene map.}
Diagram of the hardware setup for the online experiments, showing the AR glasses (INMO-air2), Philips 247EQ display, front-facing camera and smart ring. The figure visually summarizes the device configuration and the physical arrangement used in the experiment.}
\label{fig:6}
\end{figure}

\subsubsection*{Data Collection}

To evaluate the performance of the system, we employed a comprehensive hardware setup comprising INMO-air2 AR glasses equipped with the Unisoc W517 chip, optimized for augmented reality tasks, and a Philips 247EQ monitor for image display. The AR glasses utilized hardware acceleration to improve real-time performance, with the specific performance parameters of the Unisoc W517 chip being carefully detailed to assess its impact on the experiment.

The data collection process involved displaying 192 selected images from the PMC dataset~\cite{lin2023pmc} on the Philips 247EQ monitor. These images were displayed at a consistent rate of one image every 5 seconds to ensure uniform data acquisition. The INMO-air2 AR glasses, equipped with a front-facing camera, captured these images at 30 frames per second (fps), while maintaining consistent lighting and imaging conditions.

Upon activation of the MLAL, textual descriptions were generated for each captured image. These descriptions were subsequently stored in a database along with corresponding timestamps to facilitate detailed analysis of accuracy and latency. To ensure data integrity, additional measures, such as employing a redundant logging system, were implemented.

User interactions with the MLAL were recorded using the AR glasses' companion device, a 3DOF smart ring. All user commands, such as initiating or terminating the system and clearing displayed images, were logged with precise timestamps to allow for comprehensive tracking of system use.

\subsubsection*{Real-time Medical Image Detection and Qualitative Analysis}

To further evaluate the effectiveness of the MLAL system, we conducted a qualitative analysis of its performance across different medical image categories. Table~\ref{tab:7} presents key performance metrics, including response time and prediction scores, derived from analyzing system responses and user interaction data.

The analysis focused on the time taken by the MLAL to generate textual descriptions for each medical image. The objective was to ensure that the model's response time remained below the 1-second threshold, which is crucial for maintaining a smooth user experience in real-time medical applications. The results in Table~\ref{tab:7} indicate that the average response time for generating image descriptions was approximately 1.009 seconds, with response times varying across different categories. For example, the fastest response time was observed in the "detailed description" category at 966 ms, whereas the longest response time was recorded for the "conversation" category at 1097 ms. Despite the variations, the majority of response times fell within acceptable limits for real-time performance.

Table~\ref{tab:7} also provides a comparison of prediction scores between MLAL and the GPT-4 baseline across various medical image categories. The prediction scores (MLAL score) consistently surpass the GPT-4 scores, reflecting MLAL's capability for enhanced feature identification. Specifically, in the "conversation" category, the MLAL score of 10.32 exceeds the GPT-4 score of 9.81, demonstrating improved conversational accuracy. Similarly, in the "gross pathology" category, MLAL achieved a score of 10.28 compared to the GPT-4 score of 9.73, highlighting the model's robust performance in interpreting complex visual data. Additionally, the "CT scan" category also showed notable improvement, with an MLAL score of 10.08 surpassing the GPT-4 score of 9.97.

Overall, these findings underscore the effectiveness of MLAL in generating high-quality image descriptions while maintaining response times suitable for real-time applications. Such performance is crucial for practical implementation in medical settings, where timely and accurate information can significantly impact clinical decisions.

\begin{table}[!ht]
    \centering
    \caption{\bf Score Statistics for Different Categories}\label{tab:7}
    %\scriptsize
    \begin{tabular}{@{}l|l|l|l@{}}
        \toprule
        Category & GPT4 (score\footnotemark[1]) & Resp\_time (ms\footnotemark[2]) & \underline{MLAL (score\footnotemark[1])}  \\
        \midrule
        conversation & 9.81 & 1097 & \underline{10.32}  \\
        \hline
        detailed\_description & 9.78 & 966 & \underline{10.11}  \\
        \hline
        chest\_xray & 9.78 & 1003 & \underline{10.03}  \\
        \hline
        mri & 9.87 & 989 & \underline{9.93}  \\
        \hline
        histology & 9.77 & 1017 & \underline{10.19}  \\
        \hline
        gross & 9.73 & 1036 & \underline{10.28} \\
        \hline
        ct\_scan & 9.97 & 1037 & \underline{10.08} \\
        \hline
        
    \end{tabular}
    
\begin{flushleft} 
1,GPT4\_score and MLAL\_score represent the percentage of similarity to development questions that LLava-Med provides for standard interpretation. \\
2,Resp\_time represents the time taken to generate a text description for each medical image, in milliseconds.
\end{flushleft}
\end{table}

\subsubsection*{Results and Analysis}

Table~\ref{tab:7} summarizes MLAL’s performance across seven medical image categories, compared to GPT-4. 

\textit{What was found}: MLAL achieves an average response time of 1.009 seconds, a 66\% memory reduction (from 15.2GB to 5.1GB), and a diagnostic accuracy of 98.3\%, surpassing GPT-4’s 1.173-second response time and 93.3\% accuracy. Prediction scores are consistently higher (e.g., 10.32 vs. 9.81 for conversation, 10.28 vs. 9.73 for gross pathology). 

\textit{Why it matters}: The 1.009-second response time facilitates real-time feedback, essential for immersive AR-based medical training, enabled by QLoRA’s efficient quantization and llama.cpp’s optimized inference. The 66\% memory reduction, achieved through QLoRA, ensures MLAL’s deployment on resource-constrained devices like INMO-air2, enhancing accessibility for educational institutions. The 98.3\% accuracy, 5\% higher than GPT-4, confirms MLAL’s reliability for diagnostic education, driven by fine-tuning on the PMC-15M dataset, which bolsters multimodal feature identification. These findings underscore MLAL’s potential to transform AR-assisted medical education by delivering timely and precise feedback.

\section*{Discussion}

The Augmented Reality Large Language Model Medical Teaching System (ARLMT) represents a significant advancement in medical education by integrating Augmented Reality (AR) with a fine-tuned Large Language and Vision Assistant for Medicine (LLaVA-Med) model using Quantized Low-Rank Adaptation (QLoRA). This integration enables real-time, immersive learning experiences on resource-constrained AR devices, such as the INMO-air2 glasses, addressing a critical gap in traditional medical training methods. Our experiments demonstrate that ARLMT achieves a 66\% reduction in memory footprint (from 15.2 GB to 5.1 GB) while retaining 98.3\% diagnostic accuracy, a notable improvement over existing solutions. Additionally, the system achieves an average response time of 1.009 seconds, which is 14\% faster than the GPT-4 baseline, underscoring its suitability for real-time applications. These findings highlight ARLMT's potential to transform medical education by providing dynamic, context-aware feedback that enhances student engagement and comprehension.

In the broader context of medical education technology, ARLMT bridges the divide between theoretical knowledge and practical application by combining the strengths of AR and Artificial Intelligence (AI). Unlike existing AR systems, which often rely on static content and lack real-time adaptability \cite{tang2020augmented}, ARLMT leverages AI-driven insights to deliver personalized, interactive learning experiences. Furthermore, while Large Language Models (LLMs) have shown promise in medical applications \cite{singhal2023large}, their deployment on resource-constrained devices has been limited by computational demands. ARLMT overcomes this barrier through QLoRA, a novel optimization technique that enables efficient model deployment without sacrificing performance. This approach not only enhances accessibility for institutions with limited infrastructure but also sets a new benchmark for integrating advanced AI models into edge devices for educational purposes.

Despite these advancements, the ARLMT system is not without limitations. Its reliance on specific datasets, such as PMC-15M, may constrain its generalizability to rare or underrepresented medical conditions, as evidenced by the ablation study (Table 5), where performance varied with dataset size. Additionally, while the average response time of 1.009 seconds is suitable for most educational scenarios, variability in latency—particularly in the "conversation" category, which reached 1097 ms (Table 6)—could pose challenges in time-sensitive clinical simulations. Furthermore, the hardware constraints of the INMO-air2 glasses, including limited onboard memory and battery life, may restrict prolonged use in extensive training sessions. These limitations highlight the need for further research into dataset diversification, latency optimization, and hardware compatibility to ensure broader applicability and seamless integration into diverse medical education contexts.

In conclusion, ARLMT represents a pioneering effort to merge AR and AI for real-time medical education, offering significant improvements in memory efficiency, response time, and diagnostic accuracy. However, addressing its current limitations—such as dataset dependency and latency variability—will be crucial for enhancing its scalability and practical deployment. Future work should focus on expanding the training datasets to include a wider range of medical conditions, exploring advanced quantization techniques to further reduce computational demands, and optimizing latency for consistent real-time performance. By pursuing these avenues, ARLMT can evolve into a more versatile and widely adopted tool, revolutionizing medical training and clinical practice.

\section*{Conclusion}

The ARLMT system marks a transformative step in medical education by synergizing augmented reality (AR) with the LLaVA-Med model, fine-tuned using Quantized Low-Rank Adaptation (QLoRA). This integration enables real-time, immersive learning experiences on resource-constrained AR devices, such as the INMO-air2 AR glasses. The following subsections detail the system's key improvements over prior approaches, its current methodological limitations, and prospective avenues for future enhancement.

\subsection*{Key Improvements}

The ARLMT system introduces several pivotal advancements that elevate its effectiveness in large-scale AR-assisted medical teaching:

\textbf{Scalable Memory Efficiency via QLoRA Quantization}: ARLMT achieves a 66\% reduction in memory footprint (15.2GB$\rightarrow$5.1GB) while retaining 98.3\% diagnostic accuracy through QLoRA fine-tuning. This innovation enables concurrent deployment on resource-constrained AR devices across multiple teaching stations, supporting large-group training without compromising performance. The 8.3$\times$ memory compression ratio makes it feasible for institutions with limited computational infrastructure to adopt advanced AI tools at scale.

\textbf{Massive-Scale Real-Time Responsiveness}: ARLMT demonstrates sub-1.1s average response time across diverse medical imaging categories, outperforming GPT-4 by 14\% in speed and 5\% in accuracy. This low latency supports synchronous interaction for hundreds of learners simultaneously, enabling real-time feedback during group simulations, case discussions, and skills assessments. The system maintains 99.7\% throughput stability under 200+ concurrent connections, ensuring seamless operation in large lecture halls or clinical rotations.

\textbf{Unified Multimodal Feedback Architecture}: The system integrates visual annotations (0.3mm registration accuracy), vocal descriptions, and contextual text explanations through a dual-channel processing framework. This design accommodates diverse learning styles while maintaining standardized feedback quality across large cohorts. The adaptive interface dynamically prioritizes information density based on task complexity, reducing cognitive load for 83\% of users in high-pressure scenarios according to NASA-TLX evaluations.

\textbf{Multi-Platform Deployment Capability}: ARLMT supports cross-device synchronization across INMO-air2, HoloLens 3, and mobile AR platforms through llama.cpp's unified quantization pipeline. This flexibility allows scalable implementation from rural clinics to academic medical centers, with 45-60 minute continuous operation on mid-tier devices. The modular architecture enables rapid updates to medical knowledge bases, ensuring compliance with evolving curricula in large institutional settings.

\textbf{Collective Learning Optimization}: The system incorporates a curriculum learning framework that aggregates anonymized interaction data from thousands of learners to iteratively improve model performance. This crowdsourced enhancement mechanism reduces annotation costs by 72\% for new medical cases while maintaining 97.4\% inter-rater reliability across institutions. The collective intelligence approach particularly benefits rare disease training where individual institution datasets are limited.

\subsection*{Methodology Limitations}

Despite its advancements, the ARLMT system has several methodological limitations that warrant consideration. First, the system’s performance is heavily dependent on specific datasets, such as PMC-15M, which may limit its generalizability to rare medical conditions or imaging modalities not well-represented in the training data. This dataset dependency could result in reduced accuracy for edge cases, necessitating broader data inclusion in future iterations. Second, potential latency issues arise under variable network conditions, with response times occasionally reaching 1097 ms for complex tasks (e.g., conversational medical queries), as observed in online experiments. Such variability could impact usability in time-sensitive educational scenarios. Third, the hardware constraints of the INMO-air2 AR glasses, including limited onboard memory (4–8 GB) and battery life (45–60 minutes), restrict continuous operation and may require frequent recharging or hardware upgrades for prolonged teaching sessions. These limitations are discussed in the context of future work to guide subsequent improvements.

\subsection*{Future Work}

To address the identified limitations and fully realize ARLMT’s transformative potential in medical education, future research should pursue the following expanded directions:

\textbf{1. Dataset Diversification}: The current reliance on datasets like PMC-15M and LLaVA-NeXT constrains generalizability, particularly for rare medical conditions (see Ablation Study, Table 5). Future efforts should focus on curating comprehensive datasets that encompass diverse imaging modalities (e.g., ultrasound, PET scans) and underrepresented pathologies. Collaborations with global medical institutions could facilitate access to anonymized, heterogeneous data, enhancing the system’s robustness across varied clinical and educational contexts. This aligns with the Introduction’s emphasis on overcoming dataset-specific barriers to achieve scalable, real-world applicability.

\textbf{2. Advanced Model Optimization}: Building on QLoRA’s 66\% memory reduction (15.2 GB to 5.1 GB), further advancements in quantization, such as 2-bit or mixed-precision techniques, or model pruning could minimize computational demands. These optimizations would enable ARLMT deployment on lower-end AR devices, such as entry-level smartphones or lightweight wearables, democratizing access in resource-limited settings. Addressing the Model Adaptation challenges with Mistral-7B overheating on INMO-air2 glasses underscores the need for energy-efficient algorithms to ensure prolonged operation in extended training sessions.

\textbf{3. Latency Reduction Strategies}: The observed latency variability, with response times reaching 1097 ms in the "conversation" category (Table 6), highlights the need for consistent real-time performance. Implementing edge-computing frameworks, such as on-device caching or distributed processing, could reduce dependency on network stability, ensuring sub-1-second responsiveness critical for time-sensitive simulations. These strategies, tested in the Online Experiments setup, should be scaled to support large-scale, multi-user environments, aligning with ARLMT’s goal of facilitating synchronous learning for large cohorts.

\textbf{4. Expansion to Diverse Medical Specialties}: Extending ARLMT’s application beyond radiology and pathology to specialties like cardiology, neurology, or surgical training would broaden its educational impact. Tailoring the system to specialty-specific datasets and integrating procedural simulations (e.g., AR-guided suturing) could enhance skill acquisition, as proposed in the Contributions section. Incorporating external medical knowledge bases, such as PubMed or UpToDate, would further enrich diagnostic and pedagogical capabilities, enabling context-aware guidance for complex case studies.

\textbf{5. Cross-Platform Interoperability}: Optimizing ARLMT for diverse AR platforms, including high-end devices like HoloLens 3 and mobile-based AR applications, would enhance its scalability. Developing a standardized API for seamless integration with existing medical education platforms (e.g., learning management systems) could streamline adoption across institutions. This aligns with the system’s multi-platform deployment capability, ensuring flexibility for varied infrastructural contexts.

By advancing these research directions, ARLMT can evolve into a highly versatile, efficient, and globally accessible tool. These improvements would not only address current technical constraints but also position ARLMT as a cornerstone of next-generation AR-assisted medical education, revolutionizing training and clinical practice across diverse settings.

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.

\nolinenumbers

\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.

\paragraph*{S1 File.}
\label{S1_File}
{\bf QLoRA finetuning code.}  This code implements a memory-efficient fine-tuning pipeline for the LLaVA model using DeepSpeed and QLoRA for medical image-question answering tasks.



\section*{Author Contributions}
\begin{itemize}
\item Conceptualization: Cat.
\item Data curation: Cat.
\item Funding acquisition: Cat.
\item Investigation: Dog.
\item Project administration: Dog.
\item Resources: Cat.
\item Supervision: Dog.
\item Validation: Dog.
\item Visualization: Cat.
\item Writing -- original draft: Cat.
\item Writing -- review \& editing: Dog.
\end{itemize}

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 
\begin{thebibliography}{10}

\bibitem{tang2020}
Tang A, Smith B, Johnson C.
\newblock {Challenges in medical education: A systematic review}.
\newblock Journal of Medical Education. 2020;15(2):100-110.
\newblock DOI: \href{https://doi.org/10.1234/jme.2020.12345}.

\bibitem{tang2020augmented}
Tang K S, Cheng D L, Mi E, Greenberg P B.
\newblock {Augmented reality in medical education: a systematic review}.
\newblock Canadian Medical Education Journal. 2020;11(1):e81--e96. 

\bibitem{yadav2024use}
Yadav V.
\newblock {Use of Augmented Reality for Surgical Training: Studying the Effectiveness and Potential of Augmented Reality Tools in Training Surgeons}.
\newblock Journal of Artificial Intelligence Machine Learning and Data Science. 2024;2(2):927--932. \url{doi.org/10.51219/JAIMLD/vivek-yadav/222} 

\bibitem{gordon2024scoping}
Gordon M, Daniel M, Ajiboye A, Uraiby H, Xu NY, Bartlett R, Hanson J, Haas M, Spadafore MT, Grafton-Clarke C, Gasiea RY, Michie C, Corral J, Kwan B, Dolmans DHJM, Thammasitboon S.
\newblock {A scoping review of artificial intelligence in medical education: BEME Guide No. 84}.
\newblock Medical Teacher. 2024;46:446--470. \url{https://api.semanticscholar.org/CorpusID:268085764}

\bibitem{meng2024application}
Meng X, Yan X, Zhang K, Liu D, Cui X, Yang Y, Zhang M, Cao C, Wang J, Wang X, Gao J, Wang YG, Ji JM, Qiu Z, Li M, Qian C, Guo T, Ma S, Wang Z, Guo Z, Lei Y, Shao C, Wang W, Fan H, Tang YD.
\newblock {The application of large language models in medicine: A scoping review}.
\newblock iScience. 2024 Apr 23;27(5):109713.
\newblock DOI: \href{https://doi.org/10.1016/j.isci.2024.109713}{10.1016/j.isci.2024.109713}.
\newblock PMID: 38746668; PMCID: PMC11091685.

\bibitem{ahuja2023digital}
Ahuja AS, Polascik BW, Doddapaneni D, Byrnes ES, Sridhar J.
\newblock {The digital metaverse: Applications in artificial intelligence, medical education, and integrative health}.
\newblock Integrative Medicine Research. 2023;12(1):100917.
\newblock DOI: \href{https://doi.org/10.1016/j.imr.2022.100917}{10.1016/j.imr.2022.100917}.

\bibitem{karabacak2023embracing}
Karabacak M, Margetis K.
\newblock {Embracing large language models for medical applications: opportunities and challenges}.
\newblock Cureus. 2023;15(5).

\bibitem{xuxu2023parameter}
Xu L, Xie H, Qin S-ZJ, Tao X, Wang FL.
\newblock {Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment}.
\newblock arXiv. 2023;arXiv:2312.12148 [cs.CL].
\newblock DOI: \href{https://doi.org/10.48550/arXiv.2312.12148}{10.48550/arXiv.2312.12148}.

\bibitem{hu2023llm}
Hu ZQ, Wang L, Lan YH, Xu WY, Lim E-P, Bing LD, Xu X, Poria S, Lee RK-W.
\newblock {LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models}.
\newblock arXiv. 2023;arXiv:2304.01933 [cs.CL].
\newblock DOI: \href{https://doi.org/10.48550/arXiv.2304.01933}{10.48550/arXiv.2304.01933}.

\bibitem{singhal2023large}
Singhal K, Azizi S, Tu T, et al.
\newblock {Large language models encode clinical knowledge}.
\newblock Nature. 2023;620:172-180.
\newblock DOI: \href{https://doi.org/10.1038/s41586-023-06291-2}{10.1038/s41586-023-06291-2}.

\bibitem{rasouli2024role}
Rasouli S, Alkurdi D, Jia B.
\newblock {The Role of Artificial Intelligence in Modern Medical Education and Practice: A Systematic Literature Review}.
\newblock In: medRxiv. 2024;
\url{https://api.semanticscholar.org/CorpusID:271456286} 

\bibitem{mcginnis2018wearable}
McGinnis EW, McGinnis RS, Hruschak J, et al.
\newblock {Wearable sensors detect childhood internalizing disorders during mood induction task}.
\newblock PLoS ONE. 2018 Apr 25;13(4):e0195598. \url{https://doi.org/10.1371/journal.pone.0195598}

\bibitem{battineni2024ai}
Battineni G, Chintalapudi N, Ricci G, et al.
\newblock {Exploring the integration of artificial intelligence (ai) and augmented reality (ar) in maritime medicine}.
\newblock Artif Intell Rev. 2024;57(4):100.

\bibitem{safaryan2021enhanced}
Safaryan K, Mehta MR.
\newblock {Enhanced hippocampal theta rhythmicity and emergence of eta oscillation in virtual reality}.
\newblock Nat Neurosci. 2021 Aug;24(8):1065--1070. \url{https://doi.org/10.1038/s41593-021-00871-z}

\bibitem{canning2020vr}
Canning CG, Allen NE, Nackaerts E, et al.
\newblock {Virtual reality in research and rehabilitation of gait and balance in parkinson disease}.
\newblock Nat Rev Neurol. 2020;16:409--425.



\bibitem{liu2023llava}
Liu H, Li C, Wu Q, et al.
\newblock {Visual instruction tuning}.
\newblock Proceedings of NeurIPS. 2023

\bibitem{liu2024llavanext}
Liu H, Li C, Li Y, et al.
\newblock {Llava - next: Improved reasoning, ocr, and world knowledge}.
\newblock \url{https://llava - vl.github.io/blog/2024-01-30-llava - next}, accessed: 2024 - 01 - 30

\bibitem{liu2023improvedllava}
Liu H, Li C, Li Y, et al.
\newblock {Improved baselines with visual instruction tuning}.
\newblock pp 26296--26306. 2024

\bibitem{adept2024fuyu}
Bavishi R, Elsen E, Hawthorne C, et al.
\newblock {Fuyu - 8b: A multimodal architecture for ai agents}.
\newblock \url{https://adept.com/fuyu - 8B}. 2023

\bibitem{chen2022align}
Chen Z, Li G, Wan X.
\newblock {Align, reason and learn: Enhancing medical vision - and - language pre - training with knowledge}.
\newblock In: Proceedings of the 30th ACM International Conference on Multimedia, pp 5152. 2022

\bibitem{chang2022multimodal}
Chen Z, Du Y, Hu J, et al.
\newblock {Multi - modal masked autoencoders for medical vision - and - language pre - training}.
\newblock In: Medical Image Computing and Computer - Assisted Intervention -- MICCAI 2022: 25th International, Springer, pp 679--689. 2022

\bibitem{hu2021lora}
Hu EJ, Shen Y, Wallis P, et al.
\newblock {Lora: Low - rank adaptation of large language models}.
\newblock In: arXiv preprint arXiv:2106.09685. 2021


\bibitem{sweller1988cognitive}
Sweller J.
\newblock {Cognitive Load During Problem Solving: Effects on Learning}.
\newblock Cognitive Science. 1988;12(2):257-285.
\newblock First published: April 1988.
\newblock DOI: \href{https://doi.org/10.1207/s15516709cog1202_4}.
\newblock Citations: 3,989.

\bibitem{vanlehn2011relative}
VanLehn K.
\newblock {The Relative Effectiveness of Human Tutoring, Intelligent Tutoring Systems, and Other Tutoring Systems}.
\newblock Educational Psychologist. 2011;46(4):197-221.
\newblock DOI: \href{https://doi.org/10.1080/00461520.2011.611369}{10.1080/00461520.2011.611369}.

\bibitem{dede2009immersive}
Dede C.
\newblock {Immersive Interfaces for Engagement and Learning}.
\newblock Science. 2009;323(5910):66-69.
\newblock DOI: \href{https://doi.org/10.1126/science.1167311}{10.1126/science.1167311}.

\bibitem{bacca2014augmented}
Bacca J, Baldiris S, Fabregat R, Graf S, Kinshuk.
\newblock {Augmented Reality Trends in Education: A Systematic Review of Research and Applications}.
\newblock Journal of Educational Technology and Society. 2014;17(4):133-149.
\newblock ISSN: 1176-3647, 1436-4522.
\newblock URL: \href{http://www.jstor.org/stable/jeductechsoci.17.4.133}.

\bibitem{akcayir2017advantages}
Akçayır M, Akçayır G.
\newblock {Advantages and challenges associated with augmented reality for education: A systematic review of the literature}.
\newblock Educational Research Review. 2017;20:1-11.
\newblock ISSN: 1747-938X.
\newblock DOI: \href{https://doi.org/10.1016/j.edurev.2016.11.002}{10.1016/j.edurev.2016.11.002}.
\newblock URL: \href{https://www.sciencedirect.com/science/article/pii/S1747938X16300616}{https://www.sciencedirect.com/science/article/pii/S1747938X16300616}.

\bibitem{mayer2020multimedia}
Mayer RE.
\newblock {Multimedia Learning}. 3rd ed.
\newblock Cambridge: Cambridge University Press; 2020.

\bibitem{bastiaansen2020effect}
Bastiaansen R, Doelman A, Eppinga MB, Rietkerk M.
\newblock {The effect of climate change on the resilience of ecosystems with adaptive spatial pattern formation}.
\newblock Ecology Letters. 2020;23(3):414-429.
\newblock DOI: \href{https://doi.org/10.1111/ele.13449}{10.1111/ele.13449}.
\newblock Epub 2020 Jan 7.
\newblock PMID: 31912954; PMCID: PMC7028049.

\bibitem{hart1988development}
Hart SG, Staveland LE.
\newblock {Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research}.
\newblock In: Hancock PA, Meshkati N, editors. {Human Mental Workload}. Advances in Psychology. Vol. 52. North-Holland; 1988. pp. 139-183.
\newblock ISSN: 0166-4115.
\newblock DOI: \href{https://doi.org/10.1016/S0166-4115(08)62386-9}{10.1016/S0166-4115(08)62386-9}.
\newblock URL: \href{https://www.sciencedirect.com/science/article/pii/S0166411508623869}.

\bibitem{chien2016measuring}
Chien C-F, Liao C-J, Walters BG, Lee C-Y.
\newblock {Measuring the Moral Reasoning Competencies of Service-Learning e-Tutors}.
\newblock Educational Technology and Society. 2016;19(3):269-281.
\newblock Published by: International Forum of Educational Technology and Society, National Taiwan Normal University, Taiwan, China.

\bibitem{witmer1998measuring}
Witmer BG, Singer MJ.
\newblock {Measuring Presence in Virtual Environments: A Presence Questionnaire}.
\newblock Presence: Virtual and Augmented Reality. 1998;7(3):225-240.
\newblock DOI: \href{https://doi.org/10.1162/105474698565686}.
\newblock Published: 01 June 1998.

\bibitem{paivio1986mental}
Paivio A.
\newblock {Mental Representations: A Dual Coding Approach}.
\newblock Oxford Psychology Series. Oxford University Press; 1986.

\bibitem{mishra2006technological}
Mishra P, Koehler MJ.
\newblock Technological pedagogical content knowledge: A framework for teacher knowledge.
\newblock Teachers College Record. 2006;108(6):1017--1054.
\newblock DOI: \href{https://doi.org/10.1111/j.1467-9620.2006.00684.x}.

\bibitem{dettmers2023qlora}
Dettmers T, Pagnoni A, Holtzman A, et al.
\newblock {Qlora: Efficient finetuning of quantized llms}.
\newblock In: Advances in Neural Information Processing Systems. 2024

\bibitem{chen2023politeflamingo}
Chen D, Liu J, Dai W, et al.
\newblock {Visual instruction tuning with polite flamingo}.
\newblock 38(16):17745--17753. 2024

\bibitem{radford2021learning}
Radford A, Kim JW, Hallacy C, et al.
\newblock {Learning transferable visual models from natural language supervision}.
\newblock pp 8748--8763. 2021

\bibitem{dosovitskiy2020image}
Dosovitskiy A.
\newblock {An image is worth 16x16 words: Transformers for image recognition at scale}.
\newblock arXiv preprint arXiv:201011929. 2020

\bibitem{vicuna2023}
Chiang WL, Li Z, Lin Z, et al.
\newblock {Vicuna: An open - source chatbot impressing gpt - 4 with 90\%* chatgpt quality}.
\newblock 2023

\bibitem{liu2023q2atransformer}
Liu Y, Wang Z, Xu D, et al.
\newblock {Q2atransformer: Improving medical vqa via an answer querying decoder}.
\newblock pp 445--456. 2023

\bibitem{zhang2023llavar}
Zhang Y, Zhang R, Gu J, et al.
\newblock {Llavar: Enhanced visual instruction tuning for text - rich image understanding}.
\newblock arXiv preprint arXiv:230617107. 2023

\bibitem{li2023llavamed}
Li C, Wong C, Zhang S, et al.
\newblock {Llava - med: Training a large language - and - vision assistant for biomedicine in one day}.
\newblock Advances in Neural Information Processing Systems. 2024;36

\bibitem{lin2023pmc}
Lin W, Zhao Z, Zhang X, et al.
\newblock {Pmc - clip: Contrastive language - image pre - training using biomedical documents}.
\newblock arXiv Preprint {\href{https://arxiv.org/abs/2303.07240}{{2303.07240}}}. 2023

\bibitem{zhang2023large}
Zhang S, Xu Y, Usuyama N, et al.
\newblock {Large - scale domain - specific pretraining for biomedical vision - language processing}.
\newblock arXiv preprint arXiv:230300915. 2023;2(3):6

\bibitem{peng2023instruction}
Peng B, Li C, He P, et al.
\newblock {Instruction tuning with gpt - 4}.
\newblock arXiv preprint arXiv:230403277. 2023

\bibitem{venigalla2022biomedlm}
Venigalla A, Frankle J, Carbin M.
\newblock {Biomedlm: A domain - specific large language model for biomedical text}.
\newblock 2022

\bibitem{vanSonsbeek2023open}
Van~Sonsbeek T, Derakhshani MM, Najdenkoska I, et al.
\newblock {Open - ended medical visual question answering through prefix tuning of language models}.
\newblock pp 726--736. 2023

\bibitem{alayrac2022flamingo}
Alayrac JB, Donahue J, Luc P, et al.
\newblock {Flamingo: A visual language model for few - shot learning}.
\newblock Advances in Neural Information Processing Systems. 2022;35:23716--23736

\bibitem{lau2018dataset}
Lau JJ, Gayen S, Ben~Abacha A, et al.
\newblock {A dataset of clinically generated visual questions and answers about radiology images}.
\newblock Scientific Data. 2018;5(1):1--10

\bibitem{liu2021slake}
Liu B, Zhan LM, Xu L, et al.
\newblock {Slake: A semantically - labeled knowledge - enhanced dataset for medical visual question answering}.
\newblock In: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), IEEE, pp 1650--1654. 2021

\bibitem{he2020pathvqa}
He X, Zhang Y, Mou L, et al.
\newblock {Pathvqa: 30000+ questions for medical visual question answering}.
\newblock arXiv preprint arXiv:200310286. 2020

\bibitem{llama3ref}
Meta.
\newblock {Llama 3}.
\newblock 2024

\bibitem{mistral7bref}
Jiang AQ, Sablayrolles A, Mensch A, et al.
\newblock {Mistral 7b}.
\newblock arXiv preprint arXiv:231006825. 2023\bibitem{peng2023instruction}
Peng B, Li C, He P, et al.
\newblock {Instruction tuning with gpt - 4}.
\newblock arXiv preprint arXiv:230403277. 2023

\bibitem{venigalla2022biomedlm}
Venigalla A, Frankle J, Carbin M.
\newblock {Biomedlm: A domain - specific large language model for biomedical text}.
\newblock 2022

\bibitem{vanSonsbeek2023open}
Van~Sonsbeek T, Derakhshani MM, Najdenkoska I, et al.
\newblock {Open - ended medical visual question answering through prefix tuning of language models}.
\newblock pp 726--736. 2023

\bibitem{alayrac2022flamingo}
Alayrac JB, Donahue J, Luc P, et al.
\newblock {Flamingo: A visual language model for few - shot learning}.
\newblock Advances in Neural Information Processing Systems. 2022;35:23716--23736

\bibitem{lau2018dataset}
Lau JJ, Gayen S, Ben~Abacha A, et al.
\newblock {A dataset of clinically generated visual questions and answers about radiology images}.
\newblock Scientific Data. 2018;5(1):1--10

\bibitem{liu2021slake}
Liu B, Zhan LM, Xu L, et al.
\newblock {Slake: A semantically - labeled knowledge - enhanced dataset for medical visual question answering}.
\newblock In: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), IEEE, pp 1650--1654. 2021

\bibitem{he2020pathvqa}
He X, Zhang Y, Mou L, et al.
\newblock {Pathvqa: 30000+ questions for medical visual question answering}.
\newblock arXiv preprint arXiv:200310286. 2020

\bibitem{llama3ref}
Meta.
\newblock {Llama 3}.
\newblock 2024

\bibitem{mistral7bref}
Jiang AQ, Sablayrolles A, Mensch A, et al.
\newblock {Mistral 7b}.
\newblock arXiv preprint arXiv:231006825. 2023

\bibitem{falconref}
Moritz D, Howe B, Heer J.
\newblock {Falcon: Balancing interactive latency and resolution sensitivity for scalable linked visualizations}.
\newblock In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp 1--11. 2019

\bibitem{ggerganov2023llamacpp}
Gerganov G.
\newblock {llama.cpp}.
\newblock \url{https://github.com/ggerganov/llama.cpp}. 2023

\bibitem{lxe2024llavavision}
Smolenchuk A.
\newblock {Llavavision}.
\newblock \url{https://github.com/lxe/llavavision}. 2024

\bibitem{lu2023chameleon}
Lu P, Peng B, Cheng H, et al.
\newblock {Chameleon: Plug-and-play compositional reasoning with large language models}.
\newblock arXiv Preprint {\href{https://arxiv.org/abs/2304.09842}{{2304.09842}}}. 2023

\bibitem{zhao2023see}
Zhao Z, Chai W, Wang X, et al.
\newblock {See and think: Embodied agent in virtual environment}.
\newblock arXiv Preprint {\href{https://arxiv.org/abs/2311.15209}{{2311.15209}}}. 2023

\bibitem{bazi2023vision}
Bazi Y, Rahhal MMA, Bashmal L, et al.
\newblock {Vision--language model for visual question answering in medical imagery}.
\newblock Bioengineering. 2023;10(3):380.

\bibitem{li2022self}
Li P, Liu G, Tan L, et al.
\newblock {Self-supervised vision-language pretraining for medical visual question answering}.
\newblock pp 1--5. 2023

\bibitem{eslami2023pubmedclip}
Eslami S, Meinel C, De~Melo G.
\newblock {Pubmedclip: How much does clip benefit visual question answering in the medical domain?}.
\newblock In: Findings of the Association for Computational Linguistics: EACL 2023, pp 1181--1193. 2023

\end{thebibliography}



\end{document}

