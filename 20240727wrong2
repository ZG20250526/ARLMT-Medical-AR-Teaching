(llava) guoyunfei@tme-GS4840:~/projects/LLaVA-main/llava$ TRANSFORMERS_OFFLINE=1 HF_HUB_OFFLINE=1 . pretrain.sh 
[2024-07-27 17:58:09,539] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:11,396] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-27 17:58:11,396] [INFO] [runner.py:571:main] cmd = /home/guoyunfei/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py --deepspeed /home/guoyunfei/projects/LLaVA-main/scripts/zero2.json --model_name_or_path /data0/GYF/offline_model/liuhaotian/llava-v1.6-vicuna-7b --version plain --data_path /data0/GYF/data/llava/LLaVA-CC3M-Pretrain-595K/blip_laion_cc_sbu_558k.json --image_folder /data0/GYF/data/llava/LLaVA-CC3M-Pretrain-595K/images/images --vision_tower ./openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir /home/guoyunfei/projects/LLaVA-main/llava/checkpoints/llava-v1.6-7b-pretrain --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-07-27 17:58:14,192] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:16,065] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-27 17:58:16,065] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-27 17:58:16,065] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-27 17:58:16,065] [INFO] [launch.py:163:main] dist_world_size=8
[2024-07-27 17:58:16,065] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-27 17:58:19,881] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:20,095] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:20,107] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:20,109] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:20,263] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:20,275] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:20,487] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:20,543] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 17:58:21,250] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 17:58:21,313] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 17:58:21,314] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-27 17:58:21,459] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 17:58:21,506] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 17:58:21,623] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 17:58:21,826] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 17:58:21,838] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-27 17:58:21,981] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4259, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    if metadata.get("format") not in ["pt", "tf", "flax"]:
AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4259, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    if metadata.get("format") not in ["pt", "tf", "flax"]:
AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4259, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    if metadata.get("format") not in ["pt", "tf", "flax"]:
AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4259, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    if metadata.get("format") not in ["pt", "tf", "flax"]:
AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4259, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    if metadata.get("format") not in ["pt", "tf", "flax"]:
AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4259, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    if metadata.get("format") not in ["pt", "tf", "flax"]:
AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4259, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    if metadata.get("format") not in ["pt", "tf", "flax"]:
AttributeError: 'NoneType' object has no attribute 'get'
Loading checkpoint shards:   0%|                                                                                        | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4259, in _load_pretrained_model
    state_dict = load_state_dict(shard_file)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    if metadata.get("format") not in ["pt", "tf", "flax"]:
AttributeError: 'NoneType' object has no attribute 'get'
[2024-07-27 17:58:46,115] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4120357
[2024-07-27 17:58:46,146] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4120358
[2024-07-27 17:58:46,178] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4120359
[2024-07-27 17:58:46,200] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4120360
[2024-07-27 17:58:46,220] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4120361
[2024-07-27 17:58:46,221] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4120362
[2024-07-27 17:58:46,243] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4120363
[2024-07-27 17:58:46,269] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4120364
[2024-07-27 17:58:46,290] [ERROR] [launch.py:321:sigkill_handler] ['/home/guoyunfei/.conda/envs/llava/bin/python', '-u', '/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py', '--local_rank=7', '--deepspeed', '/home/guoyunfei/projects/LLaVA-main/scripts/zero2.json', '--model_name_or_path', '/data0/GYF/offline_model/liuhaotian/llava-v1.6-vicuna-7b', '--version', 'plain', '--data_path', '/data0/GYF/data/llava/LLaVA-CC3M-Pretrain-595K/blip_laion_cc_sbu_558k.json', '--image_folder', '/data0/GYF/data/llava/LLaVA-CC3M-Pretrain-595K/images/images', '--vision_tower', './openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', '/home/guoyunfei/projects/LLaVA-main/llava/checkpoints/llava-v1.6-7b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1

