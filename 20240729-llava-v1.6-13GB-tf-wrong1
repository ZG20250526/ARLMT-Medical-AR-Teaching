[2024-07-29 16:09:21,139] [INFO] [launch.py:347:main] Process 1559466 exits successfully.
(llava) guoyunfei@tme-GS4840:~/projects/LLaVA-main/llava$ TRANSFORMERS_OFFLINE=1 HF_HUB_OFFLINE=1 . finetune.sh 
[2024-07-29 18:03:23,927] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:25,747] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-29 18:03:25,747] [INFO] [runner.py:571:main] cmd = /home/guoyunfei/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py --deepspeed /home/guoyunfei/projects/LLaVA-main/scripts/zero3.json --model_name_or_path /data0/GYF/offline_model/liuhaotian/llava-v1.6-mistral-7b --version plain --data_path /data0/GYF/data/llava-med/data/Instruction-Tuning/llava_med_instruct_10k-2024072902.json --image_folder /data0/GYF/data/llava-med/data/images --vision_tower /data0/GYF/offline_model/openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.6-7b-pretrain/mm_projector.bin --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/llava-v1.6-7b_plain-finetune --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-07-29 18:03:28,482] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:30,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-29 18:03:30,452] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-29 18:03:30,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-29 18:03:30,452] [INFO] [launch.py:163:main] dist_world_size=8
[2024-07-29 18:03:30,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-29 18:03:34,561] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:34,954] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:35,178] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:35,501] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:35,599] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:35,618] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:35,662] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 18:03:35,701] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-29 18:03:35,716] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-07-29 18:03:36,408] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 18:03:36,479] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3594, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 46, in __init__
    self.model = LlavaLlamaModel(config)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 38, in __init__
    super(LlavaLlamaModel, self).__init__(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/llava_arch.py", line 32, in __init__
    super(LlavaMetaModel, self).__init__(config)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 956, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 466, in wrapper
    self._post_init_method(module)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 1000, in _post_init_method
    self._zero_init_param(param)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 956, in _zero_init_param
    dist.broadcast(param, 0, self.get_dp_process_group())
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
    return func(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 224, in broadcast
    return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 196, in broadcast
    return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1906, in broadcast
    work = default_pg.broadcast([tensor], opts)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1333, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.18.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 172.16.9.108<58415> failed : Software caused connection abort
[2024-07-29 18:03:37,102] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 18:03:37,199] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 18:03:37,223] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 18:03:37,260] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-29 18:03:37,260] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W socket.cpp:436] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:436] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:472] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 793, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 136, in __init__
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/training_args.py", line 1483, in __post_init__
    and (self.device.type != "cuda")
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/training_args.py", line 1921, in device
    return self._setup_devices
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/training_args.py", line 1853, in _setup_devices
    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/accelerate/state.py", line 170, in __init__
    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 670, in init_distributed
    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 120, in __init__
    self.init_process_group(backend, timeout, init_method, rank, world_size)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 146, in init_process_group
    torch.distributed.init_process_group(backend,
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
[2024-07-29 18:03:37,264] [INFO] [comm.py:637:init_distributed] cdb=None
    func_return = func(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1141, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 241, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 172, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3594, in from_pretrained
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
    model = cls(config, *model_args, **model_kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 46, in __init__
    self.model = LlavaLlamaModel(config)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 38, in __init__
    super(LlavaLlamaModel, self).__init__(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/llava_arch.py", line 32, in __init__
    super(LlavaMetaModel, self).__init__(config)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 956, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 466, in wrapper
    self._post_init_method(module)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 1000, in _post_init_method
    self._zero_init_param(param)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 956, in _zero_init_param
    dist.broadcast(param, 0, self.get_dp_process_group())
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
    return func(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 224, in broadcast
    return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 196, in broadcast
    return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1906, in broadcast
    work = default_pg.broadcast([tensor], opts)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1333, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.18.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 172.16.9.108<58415> failed : Software caused connection abort
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Traceback (most recent call last):
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/home/guoyunfei/projects/LLaVA-main/llava/train/train.py", line 827, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3594, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 46, in __init__
    self.model = LlavaLlamaModel(config)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/language_model/llava_llama.py", line 38, in __init__
    super(LlavaLlamaModel, self).__init__(config)
  File "/home/guoyunfei/projects/LLaVA-main/llava/model/llava_arch.py", line 32, in __init__
    super(LlavaMetaModel, self).__init__(config)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 459, in wrapper
    f(module, *args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 956, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 466, in wrapper
    self._post_init_method(module)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 1000, in _post_init_method
    self._zero_init_param(param)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 956, in _zero_init_param
    dist.broadcast(param, 0, self.get_dp_process_group())
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
    return func(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 224, in broadcast
    return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 196, in broadcast
    return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/home/guoyunfei/.conda/envs/llava/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1906, in broadcast
    work = default_pg.broadcast([tensor], opts)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1333, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.18.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 172.16.9.108<58415> failed : Software caused connection abort
[2024-07-29 18:03:38,469] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1692516
[2024-07-29 18:03:38,470] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1692517
[2024-07-29 18:03:38,722] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1692518
[2024-07-29 18:03:38,973] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1692519
[2024-07-29 18:03:39,007] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1692520
[2024-07-29 18:03:39,253] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1692521
[2024-07-29 18:03:39,545] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1692522
[2024-07-29 18:03:39,582] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1692523
[2024-07-29 18:03:39,830] [ERROR] [launch.py:321:sigkill_handler] ['/home/guoyunfei/.conda/envs/llava/bin/python', '-u', '/home/guoyunfei/projects/LLaVA-main/llava/train/train_mem.py', '--local_rank=7', '--deepspeed', '/home/guoyunfei/projects/LLaVA-main/scripts/zero3.json', '--model_name_or_path', '/data0/GYF/offline_model/liuhaotian/llava-v1.6-mistral-7b', '--version', 'plain', '--data_path', '/data0/GYF/data/llava-med/data/Instruction-Tuning/llava_med_instruct_10k-2024072902.json', '--image_folder', '/data0/GYF/data/llava-med/data/images', '--vision_tower', '/data0/GYF/offline_model/openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', './checkpoints/llava-v1.6-7b-pretrain/mm_projector.bin', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava-v1.6-7b_plain-finetune', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1

